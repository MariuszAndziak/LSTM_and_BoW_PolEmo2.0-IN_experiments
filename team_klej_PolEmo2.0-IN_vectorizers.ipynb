{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚ùáÔ∏èVECTORIZERS #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ` 0Ô∏è‚É£Opis zadania` ### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## üáµüá± Zadanie 2.2.8** (opcjonalnie)\n",
    "\n",
    "Nowy zbi√≥r z `KLEJ` - [PolEmo2.0-IN](https://clarin-pl.eu/dspace/handle/11321/710). To zestaw recenzji online z dziedziny medycyny i hoteli. Zadaniem jest przewidzenie sentymentu recenzji.\n",
    "\n",
    "* input: `../input/klej/klej_polemo2.0-in/train.tsv`\n",
    "* models: `../models/word2vec/klej_polemo2.0-in_train`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ` 1Ô∏è‚É£Import bibliotek` ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "import spacy\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "import random\n",
    "import os\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import mlflow\n",
    "from itertools import product\n",
    "import time\n",
    "import datetime\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "import team_helper as th\n",
    "from team_helper import recall, precision, f1, get_y\n",
    "from team_helper import random_polemo2_opinion as random_opinion\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from scikitplot.metrics import plot_confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikitplot.estimators import plot_learning_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ` 3Ô∏è‚É£Usuniƒôcie niepotrzebnych ostrze≈ºe≈Ñ + ustawienie mlflow` ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"file:///home/jovyan/nlp2/shared-mlruns/team-three/mariusz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ` 4Ô∏è‚É£Pr√≥ba zapewnienia powtarzalno≈õci eksperymentu` ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value= 0\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "tf.compat.v1.set_random_seed(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ` 5Ô∏è‚É£Eksploracja zbioru` ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../input/klej/klej_polemo2.0-in/train.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Super lekarz i cz≈Çowiek przez du≈ºe C . Bardzo ...</td>\n",
       "      <td>__label__meta_plus_m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bardzo olewcze podejscie do pacjenta . Przypro...</td>\n",
       "      <td>__label__meta_minus_m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lekarz zaleci≈Ç mi kuracjƒô alternatywnƒÖ do doty...</td>\n",
       "      <td>__label__meta_amb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Konsumenci oczywi≈õcie kierujƒÖ siƒô cenƒÖ . Te l...</td>\n",
       "      <td>__label__meta_zero</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pani Doktor Iwona jest profesjonalistkƒÖ w ka≈ºd...</td>\n",
       "      <td>__label__meta_plus_m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5739</th>\n",
       "      <td>Centralne skrzyd≈Ço jest jednokondygnacyjne , z...</td>\n",
       "      <td>__label__meta_zero</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5740</th>\n",
       "      <td>Og√≥lnie w hotelu panuje balagan informacyjny -...</td>\n",
       "      <td>__label__meta_minus_m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5741</th>\n",
       "      <td>Przybyli ≈õmy z rodzinƒÖ na krotki wypoczynek . ...</td>\n",
       "      <td>__label__meta_amb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5742</th>\n",
       "      <td>Opiniƒô mo≈ºe wyraziƒá dzisiaj ka≈ºdy i jest to je...</td>\n",
       "      <td>__label__meta_zero</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5743</th>\n",
       "      <td>Apartamenty w wielkim budynku , z basenem , w ...</td>\n",
       "      <td>__label__meta_amb</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5744 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence                 target\n",
       "0     Super lekarz i cz≈Çowiek przez du≈ºe C . Bardzo ...   __label__meta_plus_m\n",
       "1     Bardzo olewcze podejscie do pacjenta . Przypro...  __label__meta_minus_m\n",
       "2     Lekarz zaleci≈Ç mi kuracjƒô alternatywnƒÖ do doty...      __label__meta_amb\n",
       "3      Konsumenci oczywi≈õcie kierujƒÖ siƒô cenƒÖ . Te l...     __label__meta_zero\n",
       "4     Pani Doktor Iwona jest profesjonalistkƒÖ w ka≈ºd...   __label__meta_plus_m\n",
       "...                                                 ...                    ...\n",
       "5739  Centralne skrzyd≈Ço jest jednokondygnacyjne , z...     __label__meta_zero\n",
       "5740  Og√≥lnie w hotelu panuje balagan informacyjny -...  __label__meta_minus_m\n",
       "5741  Przybyli ≈õmy z rodzinƒÖ na krotki wypoczynek . ...      __label__meta_amb\n",
       "5742  Opiniƒô mo≈ºe wyraziƒá dzisiaj ka≈ºdy i jest to je...     __label__meta_zero\n",
       "5743  Apartamenty w wielkim budynku , z basenem , w ...      __label__meta_amb\n",
       "\n",
       "[5744 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__label__meta_minus_m    0.380223\n",
       "__label__meta_plus_m     0.270369\n",
       "__label__meta_amb        0.181929\n",
       "__label__meta_zero       0.167479\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['target'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ` 6Ô∏è‚É£Wyodrƒôbnienie zmiennych i preprocessing` ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_oryg = df['sentence'].map(th.simple_tokens).apply(lambda x:' '.join(x))\n",
    "Xprep = df['sentence'].map(th.preprocessing).map(simple_preprocess).apply(lambda x:' '.join(x))\n",
    "y = df['target'].factorize()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(doc):\n",
    "    return ' '.join([token.lemma_ for token in th.nlp_sm(doc)])\n",
    "Xprep = Xprep.apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import texthero as hero\n",
    "df[\"clean_text\"] = hero.clean(df[\"sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECTED_X = df['clean_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECTED_X = Xprep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_NAME = 'Vectorizers_polemo2_0_IN_X_prep_t'\n",
    "SELECTED_X = Xprep\n",
    "\n",
    "# \n",
    "max_features_list = [300,500, 1000] \n",
    "\n",
    "# \n",
    "tokenizer_list = [th.polish_tokenizer_md]\n",
    "\n",
    "# \n",
    "stop_words_list = [th.get_stopwords(th.spacy_stop_words_md),[]]\n",
    "#stop_words_list = [[]]\n",
    "\n",
    "# \n",
    "vectorizers_list = [CountVectorizer, TfidfVectorizer] \n",
    "\n",
    "min_df_list = [0.01, 0.1, 1]\n",
    "\n",
    "max_df_list = [0.3, 0.5, 1.0]\n",
    "\n",
    "models_list = th.get_models(use_dummy=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ` 7Ô∏è‚É£Eksperyment` ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/108\n",
      "max_features = 300 tokenizer=polish_tokenizer_md, vectorizer=CountVectorizer, stop_words = 381, dfs=min:0.01, max:0.3\n",
      "####################\n",
      "Vectorization time: 167.49834942817688\n",
      "####################\n",
      "model = decision tree score = 0.48\n",
      "time:  1.62\n",
      "----------\n",
      "model = random forest score = 0.55\n",
      "time:  6.88\n",
      "----------\n",
      "model = extra-trees score = 0.45\n",
      "time:  6.29\n",
      "----------\n",
      "model = lightgbm score = 0.69\n",
      "time:  8.13\n",
      "----------\n",
      "model = catboost score = 0.72\n",
      "time:  33.27\n",
      "----------\n",
      "model = xgboost score = 0.72\n",
      "time:  45.41\n",
      "----------\n",
      "2/108\n",
      "max_features = 300 tokenizer=polish_tokenizer_md, vectorizer=CountVectorizer, stop_words = 381, dfs=min:0.01, max:0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['cal', 'chcieƒá', 'czas', 'kierunek', 'mieƒá', 'mo≈ºliwy', 'musiƒá', 'm√≥c', 'rok', 'wiedzieƒá'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Vectorization time: 155.29947757720947\n",
      "####################\n",
      "model = decision tree score = 0.48\n",
      "time:  1.42\n",
      "----------\n",
      "model = random forest score = 0.55\n",
      "time:  6.34\n",
      "----------\n",
      "model = extra-trees score = 0.47\n",
      "time:  5.38\n",
      "----------\n",
      "model = lightgbm score = 0.69\n",
      "time:  7.38\n",
      "----------\n",
      "model = catboost score = 0.72\n",
      "time:  29.61\n",
      "----------\n",
      "model = xgboost score = 0.72\n",
      "time:  56.84\n",
      "----------\n",
      "3/108\n",
      "max_features = 300 tokenizer=polish_tokenizer_md, vectorizer=CountVectorizer, stop_words = 381, dfs=min:0.01, max:1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['cal', 'chcieƒá', 'czas', 'kierunek', 'mieƒá', 'mo≈ºliwy', 'musiƒá', 'm√≥c', 'rok', 'wiedzieƒá'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Vectorization time: 160.43988275527954\n",
      "####################\n",
      "model = decision tree score = 0.48\n",
      "time:  1.54\n",
      "----------\n",
      "model = random forest score = 0.55\n",
      "time:  6.85\n",
      "----------\n",
      "model = extra-trees score = 0.45\n",
      "time:  5.94\n",
      "----------\n",
      "model = lightgbm score = 0.7\n",
      "time:  9.74\n",
      "----------\n",
      "model = catboost score = 0.72\n",
      "time:  33.51\n",
      "----------\n",
      "model = xgboost score = 0.72\n",
      "time:  64.27\n",
      "----------\n",
      "4/108\n",
      "max_features = 300 tokenizer=polish_tokenizer_md, vectorizer=CountVectorizer, stop_words = 381, dfs=min:0.1, max:0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['cal', 'chcieƒá', 'czas', 'kierunek', 'mieƒá', 'mo≈ºliwy', 'musiƒá', 'm√≥c', 'rok', 'wiedzieƒá'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Vectorization time: 159.390074968338\n",
      "####################\n",
      "model = decision tree score = 0.49\n",
      "time:  0.61\n",
      "----------\n",
      "model = random forest score = 0.58\n",
      "time:  5.0\n",
      "----------\n",
      "model = extra-trees score = 0.43\n",
      "time:  3.86\n",
      "----------\n",
      "model = lightgbm score = 0.61\n",
      "time:  4.66\n",
      "----------\n",
      "model = catboost score = 0.62\n",
      "time:  6.48\n",
      "----------\n",
      "model = xgboost score = 0.61\n",
      "time:  8.93\n",
      "----------\n",
      "5/108\n",
      "max_features = 300 tokenizer=polish_tokenizer_md, vectorizer=CountVectorizer, stop_words = 381, dfs=min:0.1, max:0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['cal', 'chcieƒá', 'czas', 'kierunek', 'mieƒá', 'mo≈ºliwy', 'musiƒá', 'm√≥c', 'rok', 'wiedzieƒá'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Vectorization time: 149.3212649822235\n",
      "####################\n",
      "model = decision tree score = 0.53\n",
      "time:  0.57\n",
      "----------\n",
      "model = random forest score = 0.57\n",
      "time:  4.64\n",
      "----------\n",
      "model = extra-trees score = 0.43\n",
      "time:  3.5\n",
      "----------\n",
      "model = lightgbm score = 0.61\n",
      "time:  4.38\n",
      "----------\n",
      "model = catboost score = 0.62\n",
      "time:  6.57\n",
      "----------\n",
      "model = xgboost score = 0.62\n",
      "time:  9.15\n",
      "----------\n",
      "6/108\n",
      "max_features = 300 tokenizer=polish_tokenizer_md, vectorizer=CountVectorizer, stop_words = 381, dfs=min:0.1, max:1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['cal', 'chcieƒá', 'czas', 'kierunek', 'mieƒá', 'mo≈ºliwy', 'musiƒá', 'm√≥c', 'rok', 'wiedzieƒá'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Vectorization time: 151.7968955039978\n",
      "####################\n",
      "model = decision tree score = 0.53\n",
      "time:  0.69\n",
      "----------\n",
      "model = random forest score = 0.59\n",
      "time:  5.23\n",
      "----------\n",
      "model = extra-trees score = 0.44\n",
      "time:  3.93\n",
      "----------\n",
      "model = lightgbm score = 0.61\n",
      "time:  6.8\n",
      "----------\n",
      "model = catboost score = 0.62\n",
      "time:  9.05\n",
      "----------\n",
      "model = xgboost score = 0.62\n",
      "time:  14.53\n",
      "----------\n",
      "7/108\n",
      "max_features = 300 tokenizer=polish_tokenizer_md, vectorizer=CountVectorizer, stop_words = 381, dfs=min:1, max:0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['cal', 'chcieƒá', 'czas', 'kierunek', 'mieƒá', 'mo≈ºliwy', 'musiƒá', 'm√≥c', 'rok', 'wiedzieƒá'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Vectorization time: 156.3796694278717\n",
      "####################\n",
      "model = decision tree score = 0.48\n",
      "time:  1.46\n",
      "----------\n",
      "model = random forest score = 0.55\n",
      "time:  6.15\n",
      "----------\n",
      "model = extra-trees score = 0.45\n",
      "time:  5.66\n",
      "----------\n",
      "model = lightgbm score = 0.69\n",
      "time:  6.87\n",
      "----------\n",
      "model = catboost score = 0.72\n",
      "time:  31.05\n",
      "----------\n",
      "model = xgboost score = 0.71\n",
      "time:  48.55\n",
      "----------\n",
      "8/108\n",
      "max_features = 300 tokenizer=polish_tokenizer_md, vectorizer=CountVectorizer, stop_words = 381, dfs=min:1, max:0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['cal', 'chcieƒá', 'czas', 'kierunek', 'mieƒá', 'mo≈ºliwy', 'musiƒá', 'm√≥c', 'rok', 'wiedzieƒá'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Vectorization time: 157.00638055801392\n",
      "####################\n",
      "model = decision tree score = 0.48\n",
      "time:  1.49\n",
      "----------\n",
      "model = random forest score = 0.55\n",
      "time:  6.28\n",
      "----------\n",
      "model = extra-trees score = 0.47\n",
      "time:  5.95\n",
      "----------\n",
      "model = lightgbm score = 0.69\n",
      "time:  7.6\n",
      "----------\n",
      "model = catboost score = 0.72\n",
      "time:  29.3\n",
      "----------\n",
      "model = xgboost score = 0.72\n",
      "time:  45.54\n",
      "----------\n",
      "9/108\n",
      "max_features = 300 tokenizer=polish_tokenizer_md, vectorizer=CountVectorizer, stop_words = 381, dfs=min:1, max:1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['cal', 'chcieƒá', 'czas', 'kierunek', 'mieƒá', 'mo≈ºliwy', 'musiƒá', 'm√≥c', 'rok', 'wiedzieƒá'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Vectorization time: 158.28506994247437\n",
      "####################\n",
      "model = decision tree score = 0.48\n",
      "time:  1.46\n",
      "----------\n",
      "model = random forest score = 0.56\n",
      "time:  6.33\n",
      "----------\n",
      "model = extra-trees score = 0.45\n",
      "time:  5.62\n",
      "----------\n",
      "model = lightgbm score = 0.7\n",
      "time:  7.21\n",
      "----------\n",
      "model = catboost score = 0.72\n",
      "time:  29.27\n",
      "----------\n",
      "model = xgboost score = 0.72\n",
      "time:  45.62\n",
      "----------\n",
      "10/108\n",
      "max_features = 300 tokenizer=polish_tokenizer_md, vectorizer=CountVectorizer, stop_words = 0, dfs=min:0.01, max:0.3\n",
      "####################\n",
      "Vectorization time: 147.01375198364258\n",
      "####################\n",
      "model = decision tree score = 0.48\n",
      "time:  1.58\n",
      "----------\n",
      "model = random forest score = 0.54\n",
      "time:  6.54\n",
      "----------\n",
      "model = extra-trees score = 0.44\n",
      "time:  6.03\n",
      "----------\n",
      "model = lightgbm score = 0.7\n",
      "time:  8.55\n",
      "----------\n",
      "model = catboost score = 0.72\n",
      "time:  30.29\n",
      "----------\n",
      "model = xgboost score = 0.73\n",
      "time:  45.71\n",
      "----------\n",
      "11/108\n",
      "max_features = 300 tokenizer=polish_tokenizer_md, vectorizer=CountVectorizer, stop_words = 0, dfs=min:0.01, max:0.5\n",
      "####################\n",
      "Vectorization time: 149.85908603668213\n",
      "####################\n",
      "model = decision tree score = 0.48\n",
      "time:  1.63\n",
      "----------\n",
      "model = random forest score = 0.56\n",
      "time:  6.69\n",
      "----------\n",
      "model = extra-trees score = 0.44\n",
      "time:  5.87\n",
      "----------\n",
      "model = lightgbm score = 0.71\n",
      "time:  9.23\n",
      "----------\n",
      "model = catboost score = 0.73\n",
      "time:  31.84\n",
      "----------\n",
      "model = xgboost score = 0.73\n",
      "time:  56.44\n",
      "----------\n",
      "12/108\n",
      "max_features = 300 tokenizer=polish_tokenizer_md, vectorizer=CountVectorizer, stop_words = 0, dfs=min:0.01, max:1.0\n",
      "####################\n",
      "Vectorization time: 156.11799430847168\n",
      "####################\n",
      "model = decision tree score = 0.54\n",
      "time:  1.87\n",
      "----------\n",
      "model = random forest score = 0.62\n",
      "time:  7.56\n",
      "----------\n",
      "model = extra-trees score = 0.44\n",
      "time:  6.68\n",
      "----------\n",
      "model = lightgbm score = 0.72\n",
      "time:  11.18\n",
      "----------\n",
      "model = catboost score = 0.74\n",
      "time:  31.75\n",
      "----------\n",
      "model = xgboost score = 0.75\n",
      "time:  46.85\n",
      "----------\n",
      "13/108\n",
      "max_features = 300 tokenizer=polish_tokenizer_md, vectorizer=CountVectorizer, stop_words = 0, dfs=min:0.1, max:0.3\n",
      "####################\n",
      "Vectorization time: 160.51572942733765\n",
      "####################\n",
      "model = decision tree score = 0.51\n",
      "time:  0.86\n",
      "----------\n",
      "model = random forest score = 0.58\n",
      "time:  5.67\n",
      "----------\n",
      "model = extra-trees score = 0.43\n",
      "time:  4.39\n",
      "----------\n",
      "model = lightgbm score = 0.64\n",
      "time:  6.89\n",
      "----------\n",
      "model = catboost score = 0.66\n",
      "time:  13.23\n",
      "----------\n",
      "model = xgboost score = 0.65\n",
      "time:  20.07\n",
      "----------\n",
      "14/108\n",
      "max_features = 300 tokenizer=polish_tokenizer_md, vectorizer=CountVectorizer, stop_words = 0, dfs=min:0.1, max:0.5\n",
      "####################\n",
      "Vectorization time: 159.98011755943298\n",
      "####################\n",
      "model = decision tree score = 0.53\n",
      "time:  0.95\n",
      "----------\n",
      "model = random forest score = 0.6\n",
      "time:  6.15\n",
      "----------\n",
      "model = extra-trees score = 0.45\n",
      "time:  4.35\n",
      "----------\n",
      "model = lightgbm score = 0.66\n",
      "time:  8.03\n",
      "----------\n",
      "model = catboost score = 0.68\n",
      "time:  12.5\n",
      "----------\n",
      "model = xgboost score = 0.67\n",
      "time:  18.02\n",
      "----------\n",
      "15/108\n",
      "max_features = 300 tokenizer=polish_tokenizer_md, vectorizer=CountVectorizer, stop_words = 0, dfs=min:0.1, max:1.0\n",
      "####################\n",
      "Vectorization time: 150.389990568161\n",
      "####################\n",
      "model = decision tree score = 0.57\n",
      "time:  1.01\n",
      "----------\n",
      "model = random forest score = 0.63\n",
      "time:  6.06\n",
      "----------\n",
      "model = extra-trees score = 0.46\n",
      "time:  5.37\n",
      "----------\n",
      "model = lightgbm score = 0.69\n",
      "time:  7.64\n",
      "----------\n",
      "model = catboost score = 0.7\n",
      "time:  13.97\n",
      "----------\n",
      "model = xgboost score = 0.71\n",
      "time:  20.13\n",
      "----------\n",
      "16/108\n",
      "max_features = 300 tokenizer=polish_tokenizer_md, vectorizer=CountVectorizer, stop_words = 0, dfs=min:1, max:0.3\n",
      "####################\n",
      "Vectorization time: 150.17296767234802\n",
      "####################\n",
      "model = decision tree score = 0.48\n",
      "time:  1.69\n",
      "----------\n",
      "model = random forest score = 0.54\n",
      "time:  6.76\n",
      "----------\n",
      "model = extra-trees score = 0.44\n",
      "time:  6.04\n",
      "----------\n",
      "model = lightgbm score = 0.7\n",
      "time:  9.78\n",
      "----------\n",
      "model = catboost score = 0.72\n",
      "time:  31.62\n",
      "----------\n",
      "model = xgboost score = 0.72\n",
      "time:  45.44\n",
      "----------\n",
      "17/108\n",
      "max_features = 300 tokenizer=polish_tokenizer_md, vectorizer=CountVectorizer, stop_words = 0, dfs=min:1, max:0.5\n",
      "####################\n",
      "Vectorization time: 150.45922255516052\n",
      "####################\n",
      "model = decision tree score = 0.48\n",
      "time:  1.74\n",
      "----------\n",
      "model = random forest score = 0.56\n",
      "time:  6.97\n",
      "----------\n",
      "model = extra-trees score = 0.44\n",
      "time:  7.2\n",
      "----------\n",
      "model = lightgbm score = 0.71\n",
      "time:  14.03\n",
      "----------\n",
      "model = catboost score = 0.73\n",
      "time:  38.61\n",
      "----------\n",
      "model = xgboost score = 0.73\n",
      "time:  55.61\n",
      "----------\n",
      "18/108\n",
      "max_features = 300 tokenizer=polish_tokenizer_md, vectorizer=CountVectorizer, stop_words = 0, dfs=min:1, max:1.0\n",
      "####################\n",
      "Vectorization time: 165.02158665657043\n",
      "####################\n",
      "model = decision tree score = 0.54\n",
      "time:  1.96\n",
      "----------\n",
      "model = random forest score = 0.62\n",
      "time:  8.01\n",
      "----------\n",
      "model = extra-trees score = 0.44\n",
      "time:  7.64\n",
      "----------\n",
      "model = lightgbm score = 0.72\n",
      "time:  15.25\n",
      "----------\n",
      "model = catboost score = 0.74\n",
      "time:  40.71\n",
      "----------\n",
      "model = xgboost score = 0.75\n",
      "time:  78.69\n",
      "----------\n",
      "19/108\n",
      "max_features = 500 tokenizer=polish_tokenizer_md, vectorizer=CountVectorizer, stop_words = 381, dfs=min:0.01, max:0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['cal', 'chcieƒá', 'czas', 'kierunek', 'mieƒá', 'mo≈ºliwy', 'musiƒá', 'm√≥c', 'rok', 'wiedzieƒá'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Vectorization time: 171.61307191848755\n",
      "####################\n",
      "model = decision tree score = 0.48\n",
      "time:  2.81\n",
      "----------\n",
      "model = random forest score = 0.55\n",
      "time:  8.57\n",
      "----------\n",
      "model = extra-trees score = 0.45\n",
      "time:  9.21\n",
      "----------\n",
      "model = lightgbm score = 0.7\n",
      "time:  13.35\n",
      "----------\n",
      "model = catboost score = 0.72\n",
      "time:  52.04\n",
      "----------\n",
      "model = xgboost score = 0.72\n",
      "time:  73.61\n",
      "----------\n",
      "20/108\n",
      "max_features = 500 tokenizer=polish_tokenizer_md, vectorizer=CountVectorizer, stop_words = 381, dfs=min:0.01, max:0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['cal', 'chcieƒá', 'czas', 'kierunek', 'mieƒá', 'mo≈ºliwy', 'musiƒá', 'm√≥c', 'rok', 'wiedzieƒá'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Vectorization time: 153.91492676734924\n",
      "####################\n",
      "model = decision tree score = 0.48\n",
      "time:  2.35\n",
      "----------\n",
      "model = random forest score = 0.55\n",
      "time:  7.6\n",
      "----------\n",
      "model = extra-trees score = 0.46\n",
      "time:  7.8\n",
      "----------\n",
      "model = lightgbm score = 0.7\n",
      "time:  10.52\n",
      "----------\n",
      "model = catboost score = 0.72\n",
      "time:  46.6\n",
      "----------\n",
      "model = xgboost score = 0.72\n",
      "time:  81.29\n",
      "----------\n",
      "21/108\n",
      "max_features = 500 tokenizer=polish_tokenizer_md, vectorizer=CountVectorizer, stop_words = 381, dfs=min:0.01, max:1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['cal', 'chcieƒá', 'czas', 'kierunek', 'mieƒá', 'mo≈ºliwy', 'musiƒá', 'm√≥c', 'rok', 'wiedzieƒá'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Vectorization time: 155.5303144454956\n",
      "####################\n",
      "model = decision tree score = 0.48\n",
      "time:  2.57\n",
      "----------\n",
      "model = random forest score = 0.55\n",
      "time:  7.97\n",
      "----------\n",
      "model = extra-trees score = 0.45\n",
      "time:  7.9\n",
      "----------\n",
      "model = lightgbm score = 0.7\n",
      "time:  8.76\n",
      "----------\n",
      "model = catboost score = 0.73\n",
      "time:  45.71\n",
      "----------\n",
      "model = xgboost score = 0.73\n",
      "time:  73.89\n",
      "----------\n",
      "22/108\n",
      "max_features = 500 tokenizer=polish_tokenizer_md, vectorizer=CountVectorizer, stop_words = 381, dfs=min:0.1, max:0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['cal', 'chcieƒá', 'czas', 'kierunek', 'mieƒá', 'mo≈ºliwy', 'musiƒá', 'm√≥c', 'rok', 'wiedzieƒá'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Vectorization time: 151.66817545890808\n",
      "####################\n",
      "model = decision tree score = 0.49\n",
      "time:  0.55\n",
      "----------\n",
      "model = random forest score = 0.58\n",
      "time:  4.55\n",
      "----------\n",
      "model = extra-trees score = 0.43\n",
      "time:  3.47\n",
      "----------\n",
      "model = lightgbm score = 0.61\n",
      "time:  4.31\n",
      "----------\n",
      "model = catboost score = 0.62\n",
      "time:  6.41\n",
      "----------\n",
      "model = xgboost score = 0.61\n",
      "time:  9.04\n",
      "----------\n",
      "23/108\n",
      "max_features = 500 tokenizer=polish_tokenizer_md, vectorizer=CountVectorizer, stop_words = 381, dfs=min:0.1, max:0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['cal', 'chcieƒá', 'czas', 'kierunek', 'mieƒá', 'mo≈ºliwy', 'musiƒá', 'm√≥c', 'rok', 'wiedzieƒá'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Vectorization time: 150.46063470840454\n",
      "####################\n",
      "model = decision tree score = 0.53\n",
      "time:  0.6\n",
      "----------\n",
      "model = random forest score = 0.57\n",
      "time:  4.55\n",
      "----------\n",
      "model = extra-trees score = 0.43\n",
      "time:  3.47\n",
      "----------\n",
      "model = lightgbm score = 0.61\n",
      "time:  4.49\n",
      "----------\n",
      "model = catboost score = 0.62\n",
      "time:  6.69\n",
      "----------\n",
      "model = xgboost score = 0.62\n",
      "time:  9.52\n",
      "----------\n",
      "24/108\n",
      "max_features = 500 tokenizer=polish_tokenizer_md, vectorizer=CountVectorizer, stop_words = 381, dfs=min:0.1, max:1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['cal', 'chcieƒá', 'czas', 'kierunek', 'mieƒá', 'mo≈ºliwy', 'musiƒá', 'm√≥c', 'rok', 'wiedzieƒá'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Vectorization time: 153.03526043891907\n",
      "####################\n",
      "model = decision tree score = 0.53\n",
      "time:  0.65\n",
      "----------\n",
      "model = random forest score = 0.59\n",
      "time:  4.82\n",
      "----------\n",
      "model = extra-trees score = 0.44\n",
      "time:  3.61\n",
      "----------\n",
      "model = lightgbm score = 0.61\n",
      "time:  5.1\n",
      "----------\n",
      "model = catboost score = 0.62\n",
      "time:  7.2\n",
      "----------\n",
      "model = xgboost score = 0.62\n",
      "time:  10.3\n",
      "----------\n",
      "25/108\n",
      "max_features = 500 tokenizer=polish_tokenizer_md, vectorizer=CountVectorizer, stop_words = 381, dfs=min:1, max:0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['cal', 'chcieƒá', 'czas', 'kierunek', 'mieƒá', 'mo≈ºliwy', 'musiƒá', 'm√≥c', 'rok', 'wiedzieƒá'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Vectorization time: 161.70775961875916\n",
      "####################\n",
      "model = decision tree score = 0.48\n",
      "time:  3.08\n",
      "----------\n",
      "model = random forest score = 0.56\n",
      "time:  9.32\n",
      "----------\n",
      "model = extra-trees score = 0.46\n",
      "time:  9.45\n",
      "----------\n",
      "model = lightgbm score = 0.7\n",
      "time:  9.66\n",
      "----------\n",
      "model = catboost score = 0.72\n",
      "time:  46.41\n",
      "----------\n",
      "model = xgboost score = 0.72\n",
      "time:  77.48\n",
      "----------\n",
      "26/108\n",
      "max_features = 500 tokenizer=polish_tokenizer_md, vectorizer=CountVectorizer, stop_words = 381, dfs=min:1, max:0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['cal', 'chcieƒá', 'czas', 'kierunek', 'mieƒá', 'mo≈ºliwy', 'musiƒá', 'm√≥c', 'rok', 'wiedzieƒá'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Vectorization time: 153.7288920879364\n",
      "####################\n",
      "model = decision tree score = 0.48\n",
      "time:  2.62\n",
      "----------\n",
      "model = random forest score = 0.55\n",
      "time:  7.41\n",
      "----------\n",
      "model = extra-trees score = 0.46\n",
      "time:  7.93\n",
      "----------\n",
      "model = lightgbm score = 0.7\n",
      "time:  8.29\n",
      "----------\n",
      "model = catboost score = 0.72\n",
      "time:  45.58\n",
      "----------\n",
      "model = xgboost score = 0.72\n",
      "time:  73.1\n",
      "----------\n",
      "27/108\n",
      "max_features = 500 tokenizer=polish_tokenizer_md, vectorizer=CountVectorizer, stop_words = 381, dfs=min:1, max:1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['cal', 'chcieƒá', 'czas', 'kierunek', 'mieƒá', 'mo≈ºliwy', 'musiƒá', 'm√≥c', 'rok', 'wiedzieƒá'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Vectorization time: 151.09973764419556\n",
      "####################\n",
      "model = decision tree score = 0.48\n",
      "time:  2.48\n",
      "----------\n",
      "model = random forest score = 0.55\n",
      "time:  7.43\n",
      "----------\n",
      "model = extra-trees score = 0.45\n",
      "time:  8.31\n",
      "----------\n",
      "model = lightgbm score = 0.7\n",
      "time:  8.92\n",
      "----------\n",
      "model = catboost score = 0.73\n",
      "time:  47.28\n",
      "----------\n",
      "model = xgboost score = 0.73\n",
      "time:  78.09\n",
      "----------\n",
      "28/108\n",
      "max_features = 500 tokenizer=polish_tokenizer_md, vectorizer=CountVectorizer, stop_words = 0, dfs=min:0.01, max:0.3\n",
      "####################\n",
      "Vectorization time: 151.48783445358276\n",
      "####################\n",
      "model = decision tree score = 0.48\n",
      "time:  2.64\n",
      "----------\n",
      "model = random forest score = 0.55\n",
      "time:  8.51\n",
      "----------\n",
      "model = extra-trees score = 0.45\n",
      "time:  9.1\n",
      "----------\n",
      "model = lightgbm score = 0.71\n",
      "time:  13.44\n",
      "----------\n",
      "model = catboost score = 0.74\n",
      "time:  57.95\n",
      "----------\n",
      "model = xgboost score = 0.73\n",
      "time:  89.95\n",
      "----------\n",
      "29/108\n",
      "max_features = 500 tokenizer=polish_tokenizer_md, vectorizer=CountVectorizer, stop_words = 0, dfs=min:0.01, max:0.5\n",
      "####################\n",
      "Vectorization time: 163.45515203475952\n",
      "####################\n",
      "model = decision tree score = 0.48\n",
      "time:  3.11\n",
      "----------\n",
      "model = random forest score = 0.56\n",
      "time:  9.04\n",
      "----------\n",
      "model = extra-trees score = 0.46\n",
      "time:  9.8\n",
      "----------\n",
      "model = lightgbm score = 0.71\n",
      "time:  13.2\n",
      "----------\n",
      "model = catboost score = 0.74\n",
      "time:  58.12\n",
      "----------\n",
      "model = xgboost score = 0.74\n",
      "time:  113.29\n",
      "----------\n",
      "30/108\n",
      "max_features = 500 tokenizer=polish_tokenizer_md, vectorizer=CountVectorizer, stop_words = 0, dfs=min:0.01, max:1.0\n",
      "####################\n",
      "Vectorization time: 149.10098838806152\n",
      "####################\n",
      "model = decision tree score = 0.54\n",
      "time:  2.61\n",
      "----------\n",
      "model = random forest score = 0.62\n",
      "time:  19.0\n",
      "----------\n",
      "model = extra-trees score = 0.46\n",
      "time:  10.56\n",
      "----------\n",
      "model = lightgbm score = 0.73\n",
      "time:  14.29\n",
      "----------\n",
      "model = catboost score = 0.74\n",
      "time:  49.94\n",
      "----------\n",
      "model = xgboost score = 0.75\n",
      "time:  95.42\n",
      "----------\n",
      "31/108\n",
      "max_features = 500 tokenizer=polish_tokenizer_md, vectorizer=CountVectorizer, stop_words = 0, dfs=min:0.1, max:0.3\n",
      "####################\n",
      "Vectorization time: 150.90783429145813\n",
      "####################\n",
      "model = decision tree score = 0.51\n",
      "time:  0.76\n",
      "----------\n",
      "model = random forest score = 0.58\n",
      "time:  6.99\n",
      "----------\n",
      "model = extra-trees score = 0.43\n",
      "time:  3.98\n",
      "----------\n",
      "model = lightgbm score = 0.64\n",
      "time:  5.92\n",
      "----------\n",
      "model = catboost score = 0.66\n",
      "time:  11.21\n",
      "----------\n",
      "model = xgboost score = 0.65\n",
      "time:  15.9\n",
      "----------\n",
      "32/108\n",
      "max_features = 500 tokenizer=polish_tokenizer_md, vectorizer=CountVectorizer, stop_words = 0, dfs=min:0.1, max:0.5\n",
      "####################\n",
      "Vectorization time: 153.84894824028015\n",
      "####################\n",
      "model = decision tree score = 0.53\n",
      "time:  0.92\n",
      "----------\n",
      "model = random forest score = 0.6\n",
      "time:  5.62\n",
      "----------\n",
      "model = extra-trees score = 0.45\n",
      "time:  4.48\n",
      "----------\n",
      "model = lightgbm score = 0.66\n",
      "time:  7.89\n",
      "----------\n",
      "model = catboost score = 0.68\n",
      "time:  13.2\n",
      "----------\n",
      "model = xgboost score = 0.67\n",
      "time:  19.1\n",
      "----------\n",
      "33/108\n",
      "max_features = 500 tokenizer=polish_tokenizer_md, vectorizer=CountVectorizer, stop_words = 0, dfs=min:0.1, max:1.0\n",
      "####################\n",
      "Vectorization time: 152.20965337753296\n",
      "####################\n",
      "model = decision tree score = 0.57\n",
      "time:  1.01\n",
      "----------\n",
      "model = random forest score = 0.63\n",
      "time:  5.97\n",
      "----------\n",
      "model = extra-trees score = 0.46\n",
      "time:  4.4\n",
      "----------\n",
      "model = lightgbm score = 0.69\n",
      "time:  7.74\n",
      "----------\n",
      "model = catboost score = 0.7\n",
      "time:  14.06\n",
      "----------\n",
      "model = xgboost score = 0.71\n",
      "time:  20.13\n",
      "----------\n",
      "34/108\n",
      "max_features = 500 tokenizer=polish_tokenizer_md, vectorizer=CountVectorizer, stop_words = 0, dfs=min:1, max:0.3\n",
      "####################\n",
      "Vectorization time: 152.8719720840454\n",
      "####################\n",
      "model = decision tree score = 0.48\n",
      "time:  2.68\n",
      "----------\n",
      "model = random forest score = 0.55\n",
      "time:  8.11\n",
      "----------\n",
      "model = extra-trees score = 0.45\n",
      "time:  8.09\n",
      "----------\n",
      "model = lightgbm score = 0.71\n",
      "time:  11.6\n",
      "----------\n",
      "model = catboost score = 0.73\n",
      "time:  50.48\n",
      "----------\n",
      "model = xgboost score = 0.73\n",
      "time:  77.73\n",
      "----------\n",
      "35/108\n",
      "max_features = 500 tokenizer=polish_tokenizer_md, vectorizer=CountVectorizer, stop_words = 0, dfs=min:1, max:0.5\n",
      "####################\n",
      "Vectorization time: 150.9220962524414\n",
      "####################\n",
      "model = decision tree score = 0.48\n",
      "time:  2.67\n",
      "----------\n",
      "model = random forest score = 0.57\n",
      "time:  8.13\n",
      "----------\n",
      "model = extra-trees score = 0.44\n",
      "time:  8.33\n",
      "----------\n",
      "model = lightgbm score = 0.71\n",
      "time:  10.99\n",
      "----------\n",
      "model = catboost score = 0.74\n",
      "time:  48.65\n",
      "----------\n",
      "model = xgboost score = 0.74\n",
      "time:  76.34\n",
      "----------\n",
      "36/108\n",
      "max_features = 500 tokenizer=polish_tokenizer_md, vectorizer=CountVectorizer, stop_words = 0, dfs=min:1, max:1.0\n",
      "####################\n",
      "Vectorization time: 153.85909223556519\n",
      "####################\n",
      "model = decision tree score = 0.54\n",
      "time:  2.62\n",
      "----------\n",
      "model = random forest score = 0.62\n",
      "time:  8.08\n",
      "----------\n",
      "model = extra-trees score = 0.46\n",
      "time:  8.2\n",
      "----------\n",
      "model = lightgbm score = 0.73\n",
      "time:  11.45\n",
      "----------\n",
      "model = catboost score = 0.74\n",
      "time:  49.03\n",
      "----------\n",
      "model = xgboost score = 0.75\n",
      "time:  75.33\n",
      "----------\n",
      "37/108\n",
      "max_features = 1000 tokenizer=polish_tokenizer_md, vectorizer=CountVectorizer, stop_words = 381, dfs=min:0.01, max:0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['cal', 'chcieƒá', 'czas', 'kierunek', 'mieƒá', 'mo≈ºliwy', 'musiƒá', 'm√≥c', 'rok', 'wiedzieƒá'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Vectorization time: 154.04586505889893\n",
      "####################\n",
      "model = decision tree score = 0.48\n",
      "time:  6.63\n",
      "----------\n",
      "model = random forest score = 0.53\n",
      "time:  11.37\n",
      "----------\n",
      "model = extra-trees score = 0.45\n",
      "time:  13.78\n",
      "----------\n",
      "model = lightgbm score = 0.7\n",
      "time:  11.29\n",
      "----------\n",
      "model = catboost score = 0.74\n",
      "time:  87.97\n",
      "----------\n",
      "model = xgboost score = 0.74\n",
      "time:  214.8\n",
      "----------\n",
      "38/108\n",
      "max_features = 1000 tokenizer=polish_tokenizer_md, vectorizer=CountVectorizer, stop_words = 381, dfs=min:0.01, max:0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['cal', 'chcieƒá', 'czas', 'kierunek', 'mieƒá', 'mo≈ºliwy', 'musiƒá', 'm√≥c', 'rok', 'wiedzieƒá'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Vectorization time: 157.92006158828735\n",
      "####################\n",
      "model = decision tree score = 0.48\n",
      "time:  5.8\n",
      "----------\n",
      "model = random forest score = 0.52\n",
      "time:  9.94\n",
      "----------\n",
      "model = extra-trees score = 0.43\n",
      "time:  12.29\n",
      "----------\n",
      "model = lightgbm score = 0.71\n",
      "time:  12.43\n",
      "----------\n",
      "model = catboost score = 0.73\n",
      "time:  92.89\n",
      "----------\n",
      "model = xgboost score = 0.73\n",
      "time:  154.25\n",
      "----------\n",
      "39/108\n",
      "max_features = 1000 tokenizer=polish_tokenizer_md, vectorizer=CountVectorizer, stop_words = 381, dfs=min:0.01, max:1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['cal', 'chcieƒá', 'czas', 'kierunek', 'mieƒá', 'mo≈ºliwy', 'musiƒá', 'm√≥c', 'rok', 'wiedzieƒá'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Vectorization time: 158.49378848075867\n",
      "####################\n",
      "model = decision tree score = 0.48\n",
      "time:  6.85\n",
      "----------\n",
      "model = random forest score = 0.53\n",
      "time:  11.29\n",
      "----------\n",
      "model = extra-trees score = 0.44\n",
      "time:  14.51\n",
      "----------\n",
      "model = lightgbm score = 0.71\n",
      "time:  13.16\n",
      "----------\n",
      "model = catboost score = 0.73\n",
      "time:  85.41\n",
      "----------\n",
      "model = xgboost score = 0.73\n",
      "time:  153.77\n",
      "----------\n",
      "40/108\n",
      "max_features = 1000 tokenizer=polish_tokenizer_md, vectorizer=CountVectorizer, stop_words = 381, dfs=min:0.1, max:0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['cal', 'chcieƒá', 'czas', 'kierunek', 'mieƒá', 'mo≈ºliwy', 'musiƒá', 'm√≥c', 'rok', 'wiedzieƒá'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Vectorization time: 155.48419332504272\n",
      "####################\n",
      "model = decision tree score = 0.49\n",
      "time:  0.56\n",
      "----------\n",
      "model = random forest score = 0.58\n",
      "time:  4.66\n",
      "----------\n",
      "model = extra-trees score = 0.43\n",
      "time:  3.53\n",
      "----------\n",
      "model = lightgbm score = 0.61\n",
      "time:  4.52\n",
      "----------\n",
      "model = catboost score = 0.62\n",
      "time:  7.77\n",
      "----------\n",
      "model = xgboost score = 0.61\n",
      "time:  9.85\n",
      "----------\n",
      "41/108\n",
      "max_features = 1000 tokenizer=polish_tokenizer_md, vectorizer=CountVectorizer, stop_words = 381, dfs=min:0.1, max:0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['cal', 'chcieƒá', 'czas', 'kierunek', 'mieƒá', 'mo≈ºliwy', 'musiƒá', 'm√≥c', 'rok', 'wiedzieƒá'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Vectorization time: 192.31816363334656\n",
      "####################\n",
      "model = decision tree score = 0.53\n",
      "time:  1.0\n",
      "----------\n",
      "model = random forest score = 0.57\n",
      "time:  9.46\n",
      "----------\n",
      "model = extra-trees score = 0.43\n",
      "time:  7.18\n",
      "----------\n",
      "model = lightgbm score = 0.61\n",
      "time:  10.59\n",
      "----------\n",
      "model = catboost score = 0.62\n",
      "time:  24.08\n",
      "----------\n",
      "model = xgboost score = 0.62\n",
      "time:  23.51\n",
      "----------\n",
      "42/108\n",
      "max_features = 1000 tokenizer=polish_tokenizer_md, vectorizer=CountVectorizer, stop_words = 381, dfs=min:0.1, max:1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['cal', 'chcieƒá', 'czas', 'kierunek', 'mieƒá', 'mo≈ºliwy', 'musiƒá', 'm√≥c', 'rok', 'wiedzieƒá'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Vectorization time: 157.10232305526733\n",
      "####################\n",
      "model = decision tree score = 0.53\n",
      "time:  0.66\n",
      "----------\n",
      "model = random forest score = 0.59\n",
      "time:  4.77\n",
      "----------\n",
      "model = extra-trees score = 0.44\n",
      "time:  3.68\n",
      "----------\n",
      "model = lightgbm score = 0.61\n",
      "time:  5.46\n",
      "----------\n",
      "model = catboost score = 0.62\n",
      "time:  7.32\n",
      "----------\n",
      "model = xgboost score = 0.62\n",
      "time:  9.88\n",
      "----------\n",
      "43/108\n",
      "max_features = 1000 tokenizer=polish_tokenizer_md, vectorizer=CountVectorizer, stop_words = 381, dfs=min:1, max:0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['cal', 'chcieƒá', 'czas', 'kierunek', 'mieƒá', 'mo≈ºliwy', 'musiƒá', 'm√≥c', 'rok', 'wiedzieƒá'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Vectorization time: 182.22296166419983\n",
      "####################\n",
      "model = decision tree score = 0.48\n",
      "time:  13.25\n",
      "----------\n",
      "model = random forest score = 0.52\n",
      "time:  23.08\n",
      "----------\n",
      "model = extra-trees score = 0.43\n",
      "time:  30.9\n",
      "----------\n",
      "model = lightgbm score = 0.71\n",
      "time:  30.77\n",
      "----------\n",
      "model = catboost score = 0.73\n",
      "time:  217.42\n",
      "----------\n",
      "model = xgboost score = 0.74\n",
      "time:  354.33\n",
      "----------\n",
      "44/108\n",
      "max_features = 1000 tokenizer=polish_tokenizer_md, vectorizer=CountVectorizer, stop_words = 381, dfs=min:1, max:0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['cal', 'chcieƒá', 'czas', 'kierunek', 'mieƒá', 'mo≈ºliwy', 'musiƒá', 'm√≥c', 'rok', 'wiedzieƒá'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Vectorization time: 191.22986364364624\n",
      "####################\n",
      "model = decision tree score = 0.48\n",
      "time:  6.51\n",
      "----------\n",
      "model = random forest score = 0.53\n",
      "time:  11.18\n",
      "----------\n",
      "model = extra-trees score = 0.44\n",
      "time:  14.14\n",
      "----------\n",
      "model = lightgbm score = 0.71\n",
      "time:  11.73\n",
      "----------\n",
      "model = catboost score = 0.74\n",
      "time:  84.87\n",
      "----------\n",
      "model = xgboost score = 0.74\n",
      "time:  152.12\n",
      "----------\n",
      "45/108\n",
      "max_features = 1000 tokenizer=polish_tokenizer_md, vectorizer=CountVectorizer, stop_words = 381, dfs=min:1, max:1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['cal', 'chcieƒá', 'czas', 'kierunek', 'mieƒá', 'mo≈ºliwy', 'musiƒá', 'm√≥c', 'rok', 'wiedzieƒá'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Vectorization time: 158.4516351222992\n",
      "####################\n",
      "model = decision tree score = 0.48\n",
      "time:  6.86\n",
      "----------\n",
      "model = random forest score = 0.53\n",
      "time:  12.2\n",
      "----------\n",
      "model = extra-trees score = 0.45\n",
      "time:  14.89\n",
      "----------\n",
      "model = lightgbm score = 0.7\n",
      "time:  13.24\n",
      "----------\n",
      "model = catboost score = 0.74\n",
      "time:  88.55\n",
      "----------\n",
      "model = xgboost score = 0.74\n",
      "time:  151.31\n",
      "----------\n",
      "46/108\n",
      "max_features = 1000 tokenizer=polish_tokenizer_md, vectorizer=CountVectorizer, stop_words = 0, dfs=min:0.01, max:0.3\n",
      "####################\n",
      "Vectorization time: 155.23762226104736\n",
      "####################\n",
      "model = decision tree score = 0.48\n",
      "time:  6.97\n",
      "----------\n",
      "model = random forest score = 0.53\n",
      "time:  11.62\n",
      "----------\n",
      "model = extra-trees score = 0.43\n",
      "time:  15.09\n",
      "----------\n",
      "model = lightgbm score = 0.72\n",
      "time:  17.75\n",
      "----------\n",
      "model = catboost score = 0.74\n",
      "time:  91.56\n",
      "----------\n",
      "model = xgboost score = 0.75\n",
      "time:  150.77\n",
      "----------\n",
      "47/108\n",
      "max_features = 1000 tokenizer=polish_tokenizer_md, vectorizer=CountVectorizer, stop_words = 0, dfs=min:0.01, max:0.5\n",
      "####################\n",
      "Vectorization time: 151.31845378875732\n",
      "####################\n",
      "model = decision tree score = 0.48\n",
      "time:  6.47\n",
      "----------\n",
      "model = random forest score = 0.53\n",
      "time:  11.22\n",
      "----------\n",
      "model = extra-trees score = 0.45\n",
      "time:  14.44\n",
      "----------\n",
      "model = lightgbm score = 0.72\n",
      "time:  17.34\n",
      "----------\n",
      "model = catboost score = 0.75\n",
      "time:  88.31\n",
      "----------\n",
      "model = xgboost score = 0.76\n",
      "####################\n",
      "Vectorization time: 152.65276527404785\n",
      "####################\n",
      "model = decision tree score = 0.54\n",
      "time:  5.96\n",
      "----------\n",
      "model = random forest score = 0.59\n",
      "time:  11.37\n",
      "----------\n",
      "model = extra-trees score = 0.44\n",
      "time:  13.94\n",
      "----------\n",
      "model = lightgbm score = 0.73\n",
      "time:  15.73\n",
      "----------\n",
      "model = catboost score = 0.75\n",
      "time:  89.11\n",
      "----------\n",
      "model = xgboost score = 0.77\n",
      "time:  148.02\n",
      "----------\n",
      "49/108\n",
      "max_features = 1000 tokenizer=polish_tokenizer_md, vectorizer=CountVectorizer, stop_words = 0, dfs=min:0.1, max:0.3\n",
      "####################\n",
      "Vectorization time: 150.94112515449524\n",
      "####################\n",
      "model = decision tree score = 0.51\n",
      "time:  0.83\n",
      "----------\n",
      "model = random forest score = 0.58\n",
      "time:  5.31\n",
      "----------\n",
      "model = extra-trees score = 0.43\n",
      "time:  4.0\n",
      "----------\n",
      "model = lightgbm score = 0.64\n",
      "time:  8.12\n",
      "----------\n",
      "model = catboost score = 0.66\n",
      "time:  12.34\n",
      "----------\n",
      "model = xgboost score = 0.65\n",
      "time:  17.39\n",
      "----------\n",
      "50/108\n",
      "max_features = 1000 tokenizer=polish_tokenizer_md, vectorizer=CountVectorizer, stop_words = 0, dfs=min:0.1, max:0.5\n",
      "####################\n",
      "Vectorization time: 159.4746232032776\n",
      "####################\n",
      "model = decision tree score = 0.53\n",
      "time:  0.9\n",
      "----------\n",
      "model = random forest score = 0.6\n",
      "time:  5.58\n",
      "----------\n",
      "model = extra-trees score = 0.45\n",
      "time:  4.37\n",
      "----------\n",
      "model = lightgbm score = 0.66\n",
      "time:  7.78\n",
      "----------\n",
      "model = catboost score = 0.68\n",
      "time:  13.83\n",
      "----------\n",
      "model = xgboost score = 0.67\n",
      "time:  18.48\n",
      "----------\n",
      "51/108\n",
      "max_features = 1000 tokenizer=polish_tokenizer_md, vectorizer=CountVectorizer, stop_words = 0, dfs=min:0.1, max:1.0\n",
      "####################\n",
      "Vectorization time: 153.94670295715332\n",
      "####################\n",
      "model = decision tree score = 0.57\n",
      "time:  1.01\n",
      "----------\n",
      "model = random forest score = 0.63\n",
      "time:  5.9\n",
      "----------\n",
      "model = extra-trees score = 0.46\n",
      "time:  4.65\n",
      "----------\n",
      "model = lightgbm score = 0.69\n",
      "time:  7.47\n",
      "----------\n",
      "model = catboost score = 0.7\n",
      "time:  13.99\n",
      "----------\n",
      "model = xgboost score = 0.71\n",
      "time:  19.5\n",
      "----------\n",
      "52/108\n",
      "max_features = 1000 tokenizer=polish_tokenizer_md, vectorizer=CountVectorizer, stop_words = 0, dfs=min:1, max:0.3\n",
      "####################\n",
      "Vectorization time: 154.85264420509338\n",
      "####################\n",
      "model = decision tree score = 0.48\n",
      "time:  7.86\n",
      "----------\n",
      "model = random forest score = 0.53\n",
      "time:  13.37\n",
      "----------\n",
      "model = extra-trees score = 0.43\n",
      "time:  14.35\n",
      "----------\n",
      "model = lightgbm score = 0.72\n",
      "time:  15.9\n",
      "----------\n",
      "model = catboost score = 0.74\n",
      "time:  91.97\n",
      "----------\n",
      "model = xgboost score = 0.75\n",
      "time:  208.28\n",
      "----------\n",
      "53/108\n",
      "max_features = 1000 tokenizer=polish_tokenizer_md, vectorizer=CountVectorizer, stop_words = 0, dfs=min:1, max:0.5\n",
      "####################\n",
      "Vectorization time: 212.93985271453857\n",
      "####################\n",
      "model = decision tree score = 0.48\n",
      "time:  9.9\n",
      "----------\n",
      "model = random forest score = 0.54\n",
      "time:  16.73\n",
      "----------\n",
      "model = extra-trees score = 0.44\n",
      "time:  22.15\n",
      "----------\n",
      "model = lightgbm score = 0.72\n",
      "time:  39.66\n",
      "----------\n",
      "model = catboost score = 0.75\n",
      "time:  126.28\n",
      "----------\n",
      "model = xgboost score = 0.76\n",
      "time:  166.52\n",
      "----------\n",
      "54/108\n",
      "max_features = 1000 tokenizer=polish_tokenizer_md, vectorizer=CountVectorizer, stop_words = 0, dfs=min:1, max:1.0\n",
      "####################\n",
      "Vectorization time: 171.78563809394836\n",
      "####################\n",
      "model = decision tree score = 0.54\n",
      "time:  9.39\n",
      "----------\n",
      "model = random forest score = 0.56\n",
      "time:  16.78\n",
      "----------\n",
      "model = extra-trees score = 0.46\n",
      "time:  16.45\n",
      "----------\n",
      "model = lightgbm score = 0.73\n",
      "time:  35.85\n",
      "----------\n",
      "model = catboost score = 0.75\n",
      "time:  140.77\n",
      "----------\n",
      "model = xgboost score = 0.76\n",
      "time:  220.84\n",
      "----------\n",
      "55/108\n",
      "max_features = 300 tokenizer=polish_tokenizer_md, vectorizer=TfidfVectorizer, stop_words = 381, dfs=min:0.01, max:0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['cal', 'chcieƒá', 'czas', 'kierunek', 'mieƒá', 'mo≈ºliwy', 'musiƒá', 'm√≥c', 'rok', 'wiedzieƒá'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Vectorization time: 192.5678563117981\n",
      "####################\n",
      "model = decision tree score = 0.48\n",
      "time:  3.6\n",
      "----------\n",
      "model = random forest score = 0.55\n",
      "time:  8.99\n",
      "----------\n",
      "model = extra-trees score = 0.47\n",
      "time:  7.92\n",
      "----------\n",
      "model = lightgbm score = 0.69\n",
      "time:  30.22\n",
      "----------\n",
      "model = catboost score = 0.71\n",
      "time:  130.39\n",
      "----------\n",
      "model = xgboost score = 0.71\n",
      "time:  102.78\n",
      "----------\n",
      "56/108\n",
      "max_features = 300 tokenizer=polish_tokenizer_md, vectorizer=TfidfVectorizer, stop_words = 381, dfs=min:0.01, max:0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['cal', 'chcieƒá', 'czas', 'kierunek', 'mieƒá', 'mo≈ºliwy', 'musiƒá', 'm√≥c', 'rok', 'wiedzieƒá'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Vectorization time: 185.97773575782776\n",
      "####################\n",
      "model = decision tree score = 0.48\n",
      "time:  2.63\n",
      "----------\n",
      "model = random forest score = 0.55\n",
      "time:  10.99\n",
      "----------\n",
      "model = extra-trees score = 0.48\n",
      "time:  6.34\n",
      "----------\n",
      "model = lightgbm score = 0.69\n",
      "time:  34.42\n",
      "----------\n",
      "model = catboost score = 0.72\n",
      "time:  129.51\n",
      "----------\n",
      "model = xgboost score = 0.72\n",
      "time:  143.33\n",
      "----------\n",
      "57/108\n",
      "max_features = 300 tokenizer=polish_tokenizer_md, vectorizer=TfidfVectorizer, stop_words = 381, dfs=min:0.01, max:1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['cal', 'chcieƒá', 'czas', 'kierunek', 'mieƒá', 'mo≈ºliwy', 'musiƒá', 'm√≥c', 'rok', 'wiedzieƒá'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Vectorization time: 205.87580728530884\n",
      "####################\n",
      "model = decision tree score = 0.48\n",
      "time:  3.74\n",
      "----------\n",
      "model = random forest score = 0.56\n",
      "time:  10.36\n",
      "----------\n",
      "model = extra-trees score = 0.46\n",
      "time:  11.62\n",
      "----------\n",
      "model = lightgbm score = 0.69\n",
      "time:  49.38\n",
      "----------\n",
      "model = catboost score = 0.71\n",
      "time:  134.99\n",
      "----------\n",
      "model = xgboost score = 0.71\n",
      "time:  177.46\n",
      "----------\n",
      "58/108\n",
      "max_features = 300 tokenizer=polish_tokenizer_md, vectorizer=TfidfVectorizer, stop_words = 381, dfs=min:0.1, max:0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['cal', 'chcieƒá', 'czas', 'kierunek', 'mieƒá', 'mo≈ºliwy', 'musiƒá', 'm√≥c', 'rok', 'wiedzieƒá'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Vectorization time: 181.2309443950653\n",
      "####################\n",
      "model = decision tree score = 0.48\n",
      "time:  1.53\n",
      "----------\n",
      "model = random forest score = 0.55\n",
      "time:  6.49\n",
      "----------\n",
      "model = extra-trees score = 0.48\n",
      "time:  4.03\n",
      "----------\n",
      "model = lightgbm score = 0.6\n",
      "time:  7.78\n",
      "----------\n",
      "model = catboost score = 0.61\n",
      "time:  18.27\n",
      "----------\n",
      "model = xgboost score = 0.61\n",
      "time:  13.9\n",
      "----------\n",
      "59/108\n",
      "max_features = 300 tokenizer=polish_tokenizer_md, vectorizer=TfidfVectorizer, stop_words = 381, dfs=min:0.1, max:0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['cal', 'chcieƒá', 'czas', 'kierunek', 'mieƒá', 'mo≈ºliwy', 'musiƒá', 'm√≥c', 'rok', 'wiedzieƒá'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Vectorization time: 167.83669805526733\n",
      "####################\n",
      "model = decision tree score = 0.52\n",
      "time:  0.93\n",
      "----------\n",
      "model = random forest score = 0.56\n",
      "time:  6.55\n",
      "----------\n",
      "model = extra-trees score = 0.47\n",
      "time:  4.05\n",
      "----------\n",
      "model = lightgbm score = 0.61\n",
      "time:  8.67\n",
      "----------\n",
      "model = catboost score = 0.62\n",
      "time:  19.0\n",
      "----------\n",
      "model = xgboost score = 0.61\n",
      "time:  12.02\n",
      "----------\n",
      "60/108\n",
      "max_features = 300 tokenizer=polish_tokenizer_md, vectorizer=TfidfVectorizer, stop_words = 381, dfs=min:0.1, max:1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['cal', 'chcieƒá', 'czas', 'kierunek', 'mieƒá', 'mo≈ºliwy', 'musiƒá', 'm√≥c', 'rok', 'wiedzieƒá'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Vectorization time: 155.10988759994507\n",
      "####################\n",
      "model = decision tree score = 0.53\n",
      "time:  0.89\n",
      "----------\n",
      "model = random forest score = 0.57\n",
      "time:  5.98\n",
      "----------\n",
      "model = extra-trees score = 0.47\n",
      "time:  3.64\n",
      "----------\n",
      "model = lightgbm score = 0.62\n",
      "time:  7.28\n",
      "----------\n",
      "model = catboost score = 0.62\n",
      "time:  15.34\n",
      "----------\n",
      "model = xgboost score = 0.62\n",
      "time:  13.24\n",
      "----------\n",
      "61/108\n",
      "max_features = 300 tokenizer=polish_tokenizer_md, vectorizer=TfidfVectorizer, stop_words = 381, dfs=min:1, max:0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['cal', 'chcieƒá', 'czas', 'kierunek', 'mieƒá', 'mo≈ºliwy', 'musiƒá', 'm√≥c', 'rok', 'wiedzieƒá'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Vectorization time: 165.9286448955536\n",
      "####################\n",
      "model = decision tree score = 0.48\n",
      "time:  2.54\n",
      "----------\n",
      "model = random forest score = 0.55\n",
      "time:  8.77\n",
      "----------\n",
      "model = extra-trees score = 0.46\n",
      "time:  7.23\n",
      "----------\n",
      "model = lightgbm score = 0.69\n",
      "time:  15.76\n",
      "----------\n",
      "model = catboost score = 0.71\n",
      "time:  83.73\n",
      "----------\n",
      "model = xgboost score = 0.71\n",
      "time:  70.6\n",
      "----------\n",
      "62/108\n",
      "max_features = 300 tokenizer=polish_tokenizer_md, vectorizer=TfidfVectorizer, stop_words = 381, dfs=min:1, max:0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['cal', 'chcieƒá', 'czas', 'kierunek', 'mieƒá', 'mo≈ºliwy', 'musiƒá', 'm√≥c', 'rok', 'wiedzieƒá'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Vectorization time: 198.36026215553284\n",
      "####################\n",
      "model = decision tree score = 0.48\n",
      "time:  2.43\n",
      "----------\n",
      "model = random forest score = 0.55\n",
      "time:  9.57\n",
      "----------\n",
      "model = extra-trees score = 0.48\n",
      "time:  7.55\n",
      "----------\n",
      "model = lightgbm score = 0.69\n",
      "time:  31.5\n",
      "----------\n",
      "model = catboost score = 0.72\n",
      "time:  131.2\n",
      "----------\n",
      "model = xgboost score = 0.72\n",
      "time:  142.12\n",
      "----------\n",
      "63/108\n",
      "max_features = 300 tokenizer=polish_tokenizer_md, vectorizer=TfidfVectorizer, stop_words = 381, dfs=min:1, max:1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['cal', 'chcieƒá', 'czas', 'kierunek', 'mieƒá', 'mo≈ºliwy', 'musiƒá', 'm√≥c', 'rok', 'wiedzieƒá'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Vectorization time: 204.16364431381226\n",
      "####################\n",
      "model = decision tree score = 0.48\n",
      "time:  2.76\n",
      "----------\n",
      "model = random forest score = 0.56\n",
      "time:  8.47\n",
      "----------\n",
      "model = extra-trees score = 0.47\n",
      "time:  8.9\n",
      "----------\n",
      "model = lightgbm score = 0.69\n",
      "time:  32.4\n",
      "----------\n",
      "model = catboost score = 0.71\n",
      "time:  116.51\n",
      "----------\n",
      "model = xgboost score = 0.72\n",
      "time:  154.2\n",
      "----------\n",
      "64/108\n",
      "max_features = 300 tokenizer=polish_tokenizer_md, vectorizer=TfidfVectorizer, stop_words = 0, dfs=min:0.01, max:0.3\n",
      "####################\n",
      "Vectorization time: 198.3984730243683\n",
      "####################\n",
      "model = decision tree score = 0.48\n",
      "time:  3.16\n",
      "----------\n",
      "model = random forest score = 0.55\n",
      "time:  12.21\n",
      "----------\n",
      "model = extra-trees score = 0.46\n",
      "time:  7.67\n",
      "----------\n",
      "model = lightgbm score = 0.7\n",
      "time:  45.53\n",
      "----------\n",
      "model = catboost score = 0.72\n",
      "time:  154.61\n",
      "----------\n",
      "model = xgboost score = 0.72\n",
      "time:  57.55\n",
      "----------\n",
      "65/108\n",
      "max_features = 300 tokenizer=polish_tokenizer_md, vectorizer=TfidfVectorizer, stop_words = 0, dfs=min:0.01, max:0.5\n",
      "####################\n",
      "Vectorization time: 152.69172406196594\n",
      "####################\n",
      "model = decision tree score = 0.53\n",
      "time:  2.92\n",
      "----------\n",
      "model = random forest score = 0.58\n",
      "time:  9.3\n",
      "----------\n",
      "model = extra-trees score = 0.46\n",
      "time:  6.79\n",
      "----------\n",
      "model = lightgbm score = 0.71\n",
      "time:  21.57\n",
      "----------\n",
      "model = catboost score = 0.72\n",
      "time:  70.04\n",
      "----------\n",
      "model = xgboost score = 0.73\n",
      "time:  59.29\n",
      "----------\n",
      "66/108\n",
      "max_features = 300 tokenizer=polish_tokenizer_md, vectorizer=TfidfVectorizer, stop_words = 0, dfs=min:0.01, max:1.0\n",
      "####################\n",
      "Vectorization time: 154.84819054603577\n",
      "####################\n",
      "model = decision tree score = 0.58\n",
      "time:  3.25\n",
      "----------\n",
      "model = random forest score = 0.62\n",
      "time:  9.66\n",
      "----------\n",
      "model = extra-trees score = 0.47\n",
      "time:  6.35\n",
      "----------\n",
      "model = lightgbm score = 0.72\n",
      "time:  31.52\n",
      "----------\n",
      "model = catboost score = 0.73\n",
      "time:  82.4\n",
      "----------\n",
      "model = xgboost score = 0.74\n",
      "time:  84.66\n",
      "----------\n",
      "67/108\n",
      "max_features = 300 tokenizer=polish_tokenizer_md, vectorizer=TfidfVectorizer, stop_words = 0, dfs=min:0.1, max:0.3\n",
      "####################\n",
      "Vectorization time: 175.51612305641174\n",
      "####################\n",
      "model = decision tree score = 0.49\n",
      "time:  1.62\n",
      "----------\n",
      "model = random forest score = 0.56\n",
      "time:  8.51\n",
      "----------\n",
      "model = extra-trees score = 0.46\n",
      "time:  5.15\n",
      "----------\n",
      "model = lightgbm score = 0.64\n",
      "time:  18.81\n",
      "----------\n",
      "model = catboost score = 0.66\n",
      "time:  38.16\n",
      "----------\n",
      "model = xgboost score = 0.64\n",
      "time:  28.06\n",
      "----------\n",
      "68/108\n",
      "max_features = 300 tokenizer=polish_tokenizer_md, vectorizer=TfidfVectorizer, stop_words = 0, dfs=min:0.1, max:0.5\n",
      "####################\n",
      "Vectorization time: 174.10083413124084\n",
      "####################\n",
      "model = decision tree score = 0.53\n",
      "time:  2.88\n",
      "----------\n",
      "model = random forest score = 0.6\n",
      "time:  14.1\n",
      "----------\n",
      "model = extra-trees score = 0.48\n",
      "time:  8.4\n",
      "----------\n",
      "model = lightgbm score = 0.66\n",
      "time:  352.81\n",
      "----------\n",
      "model = catboost score = 0.67\n",
      "time:  111.46\n",
      "----------\n",
      "model = xgboost score = 0.66\n",
      "time:  69.09\n",
      "----------\n",
      "69/108\n",
      "max_features = 300 tokenizer=polish_tokenizer_md, vectorizer=TfidfVectorizer, stop_words = 0, dfs=min:0.1, max:1.0\n",
      "####################\n",
      "Vectorization time: 310.81959342956543\n",
      "####################\n",
      "model = decision tree score = 0.57\n",
      "time:  3.52\n",
      "----------\n",
      "model = random forest score = 0.63\n",
      "time:  18.25\n",
      "----------\n",
      "model = extra-trees score = 0.52\n",
      "time:  9.08\n",
      "----------\n",
      "model = lightgbm score = 0.69\n",
      "time:  386.84\n",
      "----------\n",
      "model = catboost score = 0.7\n",
      "time:  110.65\n",
      "----------\n",
      "model = xgboost score = 0.7\n",
      "time:  76.78\n",
      "----------\n",
      "70/108\n",
      "max_features = 300 tokenizer=polish_tokenizer_md, vectorizer=TfidfVectorizer, stop_words = 0, dfs=min:1, max:0.3\n",
      "####################\n",
      "Vectorization time: 297.4100663661957\n",
      "####################\n",
      "model = decision tree score = 0.48\n",
      "time:  5.09\n",
      "----------\n",
      "model = random forest score = 0.55\n",
      "time:  16.39\n",
      "----------\n",
      "model = extra-trees score = 0.45\n",
      "time:  20.86\n",
      "----------\n",
      "model = lightgbm score = 0.7\n",
      "time:  457.05\n",
      "----------\n",
      "model = catboost score = 0.71\n",
      "time:  210.12\n",
      "----------\n",
      "model = xgboost score = 0.72\n",
      "time:  797.82\n",
      "----------\n",
      "71/108\n",
      "max_features = 300 tokenizer=polish_tokenizer_md, vectorizer=TfidfVectorizer, stop_words = 0, dfs=min:1, max:0.5\n",
      "####################\n",
      "Vectorization time: 165.8633291721344\n",
      "####################\n",
      "model = decision tree score = 0.53\n",
      "time:  3.25\n",
      "----------\n",
      "model = random forest score = 0.58\n",
      "time:  10.38\n",
      "----------\n",
      "model = extra-trees score = 0.46\n",
      "time:  7.01\n",
      "----------\n",
      "model = lightgbm score = 0.71\n",
      "time:  23.72\n",
      "----------\n",
      "model = catboost score = 0.72\n",
      "time:  91.48\n",
      "----------\n",
      "model = xgboost score = 0.73\n",
      "time:  76.59\n",
      "----------\n",
      "72/108\n",
      "max_features = 300 tokenizer=polish_tokenizer_md, vectorizer=TfidfVectorizer, stop_words = 0, dfs=min:1, max:1.0\n",
      "####################\n",
      "Vectorization time: 171.88407349586487\n",
      "####################\n",
      "model = decision tree score = 0.58\n",
      "time:  3.63\n",
      "----------\n",
      "model = random forest score = 0.62\n",
      "time:  16.85\n",
      "----------\n",
      "model = extra-trees score = 0.47\n",
      "time:  13.19\n",
      "----------\n",
      "model = lightgbm score = 0.72\n",
      "time:  271.7\n",
      "----------\n",
      "model = catboost score = 0.73\n",
      "time:  182.11\n",
      "----------\n",
      "model = xgboost score = 0.74\n",
      "time:  166.26\n",
      "----------\n",
      "73/108\n",
      "max_features = 500 tokenizer=polish_tokenizer_md, vectorizer=TfidfVectorizer, stop_words = 381, dfs=min:0.01, max:0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['cal', 'chcieƒá', 'czas', 'kierunek', 'mieƒá', 'mo≈ºliwy', 'musiƒá', 'm√≥c', 'rok', 'wiedzieƒá'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Vectorization time: 202.39337491989136\n",
      "####################\n",
      "model = decision tree score = 0.48\n",
      "time:  6.79\n",
      "----------\n",
      "model = random forest score = 0.55\n",
      "time:  16.32\n",
      "----------\n",
      "model = extra-trees score = 0.46\n",
      "time:  16.4\n",
      "----------\n",
      "model = lightgbm score = 0.7\n",
      "time:  318.53\n",
      "----------\n",
      "model = catboost score = 0.72\n",
      "time:  148.64\n",
      "----------\n",
      "model = xgboost score = 0.72\n",
      "time:  694.99\n",
      "----------\n",
      "74/108\n",
      "max_features = 500 tokenizer=polish_tokenizer_md, vectorizer=TfidfVectorizer, stop_words = 381, dfs=min:0.01, max:0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['cal', 'chcieƒá', 'czas', 'kierunek', 'mieƒá', 'mo≈ºliwy', 'musiƒá', 'm√≥c', 'rok', 'wiedzieƒá'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Vectorization time: 188.32303643226624\n",
      "####################\n",
      "model = decision tree score = 0.48\n",
      "time:  4.85\n",
      "----------\n",
      "model = random forest score = 0.55\n",
      "time:  12.69\n",
      "----------\n",
      "model = extra-trees score = 0.47\n",
      "time:  11.99\n",
      "----------\n",
      "model = lightgbm score = 0.7\n",
      "time:  44.73\n",
      "----------\n",
      "model = catboost score = 0.72\n",
      "time:  150.37\n",
      "----------\n",
      "model = xgboost score = 0.72\n",
      "time:  162.93\n",
      "----------\n",
      "75/108\n",
      "max_features = 500 tokenizer=polish_tokenizer_md, vectorizer=TfidfVectorizer, stop_words = 381, dfs=min:0.01, max:1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['cal', 'chcieƒá', 'czas', 'kierunek', 'mieƒá', 'mo≈ºliwy', 'musiƒá', 'm√≥c', 'rok', 'wiedzieƒá'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Vectorization time: 187.92902088165283\n",
      "####################\n",
      "model = decision tree score = 0.49\n",
      "time:  4.72\n",
      "----------\n",
      "model = random forest score = 0.55\n",
      "time:  11.84\n",
      "----------\n",
      "model = extra-trees score = 0.46\n",
      "time:  12.22\n",
      "----------\n",
      "model = lightgbm score = 0.7\n",
      "time:  45.73\n",
      "----------\n",
      "model = catboost score = 0.72\n",
      "time:  150.93\n",
      "----------\n",
      "model = xgboost score = 0.73\n",
      "time:  189.91\n",
      "----------\n",
      "76/108\n",
      "max_features = 500 tokenizer=polish_tokenizer_md, vectorizer=TfidfVectorizer, stop_words = 381, dfs=min:0.1, max:0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['cal', 'chcieƒá', 'czas', 'kierunek', 'mieƒá', 'mo≈ºliwy', 'musiƒá', 'm√≥c', 'rok', 'wiedzieƒá'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Vectorization time: 190.41149401664734\n",
      "####################\n",
      "model = decision tree score = 0.48\n",
      "time:  1.01\n",
      "----------\n",
      "model = random forest score = 0.55\n",
      "time:  7.39\n",
      "----------\n",
      "model = extra-trees score = 0.48\n",
      "time:  4.51\n",
      "----------\n",
      "model = lightgbm score = 0.6\n",
      "time:  19.65\n",
      "----------\n",
      "model = catboost score = 0.61\n",
      "time:  26.29\n",
      "----------\n",
      "model = xgboost score = 0.61\n",
      "time:  34.32\n",
      "----------\n",
      "77/108\n",
      "max_features = 500 tokenizer=polish_tokenizer_md, vectorizer=TfidfVectorizer, stop_words = 381, dfs=min:0.1, max:0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['cal', 'chcieƒá', 'czas', 'kierunek', 'mieƒá', 'mo≈ºliwy', 'musiƒá', 'm√≥c', 'rok', 'wiedzieƒá'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Vectorization time: 189.22960376739502\n",
      "####################\n",
      "model = decision tree score = 0.52\n",
      "time:  1.02\n",
      "----------\n",
      "model = random forest score = 0.56\n",
      "time:  7.22\n",
      "----------\n",
      "model = extra-trees score = 0.47\n",
      "time:  4.64\n",
      "----------\n",
      "model = lightgbm score = 0.61\n",
      "time:  19.63\n",
      "----------\n",
      "model = catboost score = 0.62\n",
      "time:  27.04\n",
      "----------\n",
      "model = xgboost score = 0.61\n",
      "time:  33.62\n",
      "----------\n",
      "78/108\n",
      "max_features = 500 tokenizer=polish_tokenizer_md, vectorizer=TfidfVectorizer, stop_words = 381, dfs=min:0.1, max:1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['cal', 'chcieƒá', 'czas', 'kierunek', 'mieƒá', 'mo≈ºliwy', 'musiƒá', 'm√≥c', 'rok', 'wiedzieƒá'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Vectorization time: 187.35727405548096\n",
      "####################\n",
      "model = decision tree score = 0.53\n",
      "time:  1.1\n",
      "----------\n",
      "model = random forest score = 0.57\n",
      "time:  7.56\n",
      "----------\n",
      "model = extra-trees score = 0.47\n",
      "time:  4.56\n",
      "----------\n",
      "model = lightgbm score = 0.62\n",
      "time:  20.76\n",
      "----------\n",
      "model = catboost score = 0.62\n",
      "time:  28.46\n",
      "----------\n",
      "model = xgboost score = 0.62\n",
      "time:  37.73\n",
      "----------\n",
      "79/108\n",
      "max_features = 500 tokenizer=polish_tokenizer_md, vectorizer=TfidfVectorizer, stop_words = 381, dfs=min:1, max:0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['cal', 'chcieƒá', 'czas', 'kierunek', 'mieƒá', 'mo≈ºliwy', 'musiƒá', 'm√≥c', 'rok', 'wiedzieƒá'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Vectorization time: 178.84391379356384\n",
      "####################\n",
      "model = decision tree score = 0.48\n",
      "time:  4.73\n",
      "----------\n",
      "model = random forest score = 0.55\n",
      "time:  10.92\n",
      "----------\n",
      "model = extra-trees score = 0.46\n",
      "time:  9.96\n",
      "----------\n",
      "model = lightgbm score = 0.69\n",
      "time:  19.15\n",
      "----------\n",
      "model = catboost score = 0.72\n",
      "time:  135.83\n",
      "----------\n",
      "model = xgboost score = 0.72\n",
      "time:  188.81\n",
      "----------\n",
      "80/108\n",
      "max_features = 500 tokenizer=polish_tokenizer_md, vectorizer=TfidfVectorizer, stop_words = 381, dfs=min:1, max:0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['cal', 'chcieƒá', 'czas', 'kierunek', 'mieƒá', 'mo≈ºliwy', 'musiƒá', 'm√≥c', 'rok', 'wiedzieƒá'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Vectorization time: 184.4334774017334\n",
      "####################\n",
      "model = decision tree score = 0.48\n",
      "time:  4.6\n",
      "----------\n",
      "model = random forest score = 0.55\n",
      "time:  11.44\n",
      "----------\n",
      "model = extra-trees score = 0.47\n",
      "time:  12.93\n",
      "----------\n",
      "model = lightgbm score = 0.7\n",
      "time:  42.7\n",
      "----------\n",
      "model = catboost score = 0.72\n",
      "time:  150.13\n",
      "----------\n",
      "model = xgboost score = 0.72\n",
      "time:  188.5\n",
      "----------\n",
      "81/108\n",
      "max_features = 500 tokenizer=polish_tokenizer_md, vectorizer=TfidfVectorizer, stop_words = 381, dfs=min:1, max:1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['cal', 'chcieƒá', 'czas', 'kierunek', 'mieƒá', 'mo≈ºliwy', 'musiƒá', 'm√≥c', 'rok', 'wiedzieƒá'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Vectorization time: 182.3237283229828\n",
      "####################\n",
      "model = decision tree score = 0.48\n",
      "time:  4.19\n",
      "----------\n",
      "model = random forest score = 0.55\n",
      "time:  10.43\n",
      "----------\n",
      "model = extra-trees score = 0.46\n",
      "time:  11.84\n",
      "----------\n",
      "model = lightgbm score = 0.7\n",
      "time:  40.3\n",
      "----------\n",
      "model = catboost score = 0.72\n",
      "time:  149.58\n",
      "----------\n",
      "model = xgboost score = 0.72\n",
      "time:  189.33\n",
      "----------\n",
      "82/108\n",
      "max_features = 500 tokenizer=polish_tokenizer_md, vectorizer=TfidfVectorizer, stop_words = 0, dfs=min:0.01, max:0.3\n",
      "####################\n",
      "Vectorization time: 181.04524159431458\n",
      "####################\n",
      "model = decision tree score = 0.48\n",
      "time:  5.4\n",
      "----------\n",
      "model = random forest score = 0.55\n",
      "time:  12.42\n",
      "----------\n",
      "model = extra-trees score = 0.45\n",
      "time:  13.64\n",
      "----------\n",
      "model = lightgbm score = 0.71\n",
      "time:  27.25\n",
      "----------\n",
      "model = catboost score = 0.73\n",
      "time:  128.71\n",
      "----------\n",
      "model = xgboost score = 0.73\n",
      "time:  109.97\n",
      "----------\n",
      "83/108\n",
      "max_features = 500 tokenizer=polish_tokenizer_md, vectorizer=TfidfVectorizer, stop_words = 0, dfs=min:0.01, max:0.5\n",
      "####################\n",
      "Vectorization time: 172.87889051437378\n",
      "####################\n",
      "model = decision tree score = 0.51\n",
      "time:  6.53\n",
      "----------\n",
      "model = random forest score = 0.57\n",
      "time:  13.06\n",
      "----------\n",
      "model = extra-trees score = 0.46\n",
      "time:  26.73\n",
      "----------\n",
      "model = lightgbm score = 0.71\n",
      "time:  241.65\n",
      "----------\n",
      "model = catboost score = 0.73\n",
      "time:  279.2\n",
      "----------\n",
      "model = xgboost score = 0.74\n",
      "time:  202.23\n",
      "----------\n",
      "84/108\n",
      "max_features = 500 tokenizer=polish_tokenizer_md, vectorizer=TfidfVectorizer, stop_words = 0, dfs=min:0.01, max:1.0\n",
      "####################\n",
      "Vectorization time: 154.62888383865356\n",
      "####################\n",
      "model = decision tree score = 0.58\n",
      "time:  5.15\n",
      "----------\n",
      "model = random forest score = 0.63\n",
      "time:  10.78\n",
      "----------\n",
      "model = extra-trees score = 0.48\n",
      "time:  8.4\n",
      "----------\n",
      "model = lightgbm score = 0.73\n",
      "time:  25.69\n",
      "----------\n",
      "model = catboost score = 0.74\n",
      "time:  104.93\n",
      "----------\n",
      "model = xgboost score = 0.75\n",
      "time:  93.27\n",
      "----------\n",
      "85/108\n",
      "max_features = 500 tokenizer=polish_tokenizer_md, vectorizer=TfidfVectorizer, stop_words = 0, dfs=min:0.1, max:0.3\n",
      "####################\n",
      "Vectorization time: 151.84702444076538\n",
      "####################\n",
      "model = decision tree score = 0.49\n",
      "time:  1.37\n",
      "----------\n",
      "model = random forest score = 0.56\n",
      "time:  14.16\n",
      "----------\n",
      "model = extra-trees score = 0.46\n",
      "time:  4.13\n",
      "----------\n",
      "model = lightgbm score = 0.64\n",
      "time:  11.96\n",
      "----------\n",
      "model = catboost score = 0.66\n",
      "time:  26.46\n",
      "----------\n",
      "model = xgboost score = 0.64\n",
      "time:  22.59\n",
      "----------\n",
      "86/108\n",
      "max_features = 500 tokenizer=polish_tokenizer_md, vectorizer=TfidfVectorizer, stop_words = 0, dfs=min:0.1, max:0.5\n",
      "####################\n",
      "Vectorization time: 149.67484664916992\n",
      "####################\n",
      "model = decision tree score = 0.53\n",
      "time:  1.63\n",
      "----------\n",
      "model = random forest score = 0.6\n",
      "time:  8.07\n",
      "----------\n",
      "model = extra-trees score = 0.48\n",
      "time:  4.25\n",
      "----------\n",
      "model = lightgbm score = 0.66\n",
      "time:  13.54\n",
      "----------\n",
      "model = catboost score = 0.67\n",
      "time:  29.9\n",
      "----------\n",
      "model = xgboost score = 0.66\n",
      "time:  27.01\n",
      "----------\n",
      "87/108\n",
      "max_features = 500 tokenizer=polish_tokenizer_md, vectorizer=TfidfVectorizer, stop_words = 0, dfs=min:0.1, max:1.0\n",
      "####################\n",
      "Vectorization time: 149.52058339118958\n",
      "####################\n",
      "model = decision tree score = 0.57\n",
      "time:  2.0\n",
      "----------\n",
      "model = random forest score = 0.63\n",
      "time:  9.05\n",
      "----------\n",
      "model = extra-trees score = 0.52\n",
      "time:  4.67\n",
      "----------\n",
      "model = lightgbm score = 0.69\n",
      "time:  18.08\n",
      "----------\n",
      "model = catboost score = 0.7\n",
      "time:  35.83\n",
      "----------\n",
      "model = xgboost score = 0.7\n",
      "time:  31.95\n",
      "----------\n",
      "88/108\n",
      "max_features = 500 tokenizer=polish_tokenizer_md, vectorizer=TfidfVectorizer, stop_words = 0, dfs=min:1, max:0.3\n",
      "####################\n",
      "Vectorization time: 154.74858784675598\n",
      "####################\n",
      "model = decision tree score = 0.48\n",
      "time:  4.14\n",
      "----------\n",
      "model = random forest score = 0.55\n",
      "time:  9.86\n",
      "----------\n",
      "model = extra-trees score = 0.45\n",
      "time:  8.83\n",
      "----------\n",
      "model = lightgbm score = 0.71\n",
      "time:  21.16\n",
      "----------\n",
      "model = catboost score = 0.72\n",
      "time:  100.41\n",
      "----------\n",
      "model = xgboost score = 0.72\n",
      "time:  90.52\n",
      "----------\n",
      "89/108\n",
      "max_features = 500 tokenizer=polish_tokenizer_md, vectorizer=TfidfVectorizer, stop_words = 0, dfs=min:1, max:0.5\n",
      "####################\n",
      "Vectorization time: 150.61552906036377\n",
      "####################\n",
      "model = decision tree score = 0.51\n",
      "time:  4.22\n",
      "----------\n",
      "model = random forest score = 0.58\n",
      "time:  10.08\n",
      "----------\n",
      "model = extra-trees score = 0.45\n",
      "time:  8.41\n",
      "----------\n",
      "model = lightgbm score = 0.71\n",
      "time:  22.64\n",
      "----------\n",
      "model = catboost score = 0.72\n",
      "time:  103.13\n",
      "----------\n",
      "model = xgboost score = 0.74\n",
      "time:  89.62\n",
      "----------\n",
      "90/108\n",
      "max_features = 500 tokenizer=polish_tokenizer_md, vectorizer=TfidfVectorizer, stop_words = 0, dfs=min:1, max:1.0\n",
      "####################\n",
      "Vectorization time: 148.97562098503113\n",
      "####################\n",
      "model = decision tree score = 0.58\n",
      "time:  4.26\n",
      "----------\n",
      "model = random forest score = 0.63\n",
      "time:  10.42\n",
      "----------\n",
      "model = extra-trees score = 0.48\n",
      "time:  8.25\n",
      "----------\n",
      "model = lightgbm score = 0.73\n",
      "time:  25.59\n",
      "----------\n",
      "model = catboost score = 0.74\n",
      "time:  103.27\n",
      "----------\n",
      "model = xgboost score = 0.75\n",
      "time:  91.7\n",
      "----------\n",
      "91/108\n",
      "max_features = 1000 tokenizer=polish_tokenizer_md, vectorizer=TfidfVectorizer, stop_words = 381, dfs=min:0.01, max:0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['cal', 'chcieƒá', 'czas', 'kierunek', 'mieƒá', 'mo≈ºliwy', 'musiƒá', 'm√≥c', 'rok', 'wiedzieƒá'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Vectorization time: 153.79788446426392\n",
      "####################\n",
      "model = decision tree score = 0.49\n",
      "time:  7.87\n",
      "----------\n",
      "model = random forest score = 0.53\n",
      "time:  12.35\n",
      "----------\n",
      "model = extra-trees score = 0.45\n",
      "time:  14.21\n",
      "----------\n",
      "model = lightgbm score = 0.7\n",
      "time:  19.53\n",
      "----------\n",
      "model = catboost score = 0.72\n",
      "time:  141.15\n",
      "----------\n",
      "model = xgboost score = 0.73\n",
      "time:  160.24\n",
      "----------\n",
      "92/108\n",
      "max_features = 1000 tokenizer=polish_tokenizer_md, vectorizer=TfidfVectorizer, stop_words = 381, dfs=min:0.01, max:0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['cal', 'chcieƒá', 'czas', 'kierunek', 'mieƒá', 'mo≈ºliwy', 'musiƒá', 'm√≥c', 'rok', 'wiedzieƒá'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Vectorization time: 154.72181463241577\n",
      "####################\n",
      "model = decision tree score = 0.48\n",
      "time:  8.18\n",
      "----------\n",
      "model = random forest score = 0.53\n",
      "time:  12.76\n",
      "----------\n",
      "model = extra-trees score = 0.43\n",
      "time:  14.74\n",
      "----------\n",
      "model = lightgbm score = 0.7\n",
      "time:  19.8\n",
      "----------\n",
      "model = catboost score = 0.72\n",
      "time:  142.38\n",
      "----------\n",
      "model = xgboost score = 0.73\n",
      "time:  165.01\n",
      "----------\n",
      "93/108\n",
      "max_features = 1000 tokenizer=polish_tokenizer_md, vectorizer=TfidfVectorizer, stop_words = 381, dfs=min:0.01, max:1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['cal', 'chcieƒá', 'czas', 'kierunek', 'mieƒá', 'mo≈ºliwy', 'musiƒá', 'm√≥c', 'rok', 'wiedzieƒá'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Vectorization time: 154.3720109462738\n",
      "####################\n",
      "model = decision tree score = 0.48\n",
      "time:  7.91\n",
      "----------\n",
      "model = random forest score = 0.53\n",
      "time:  12.43\n",
      "----------\n",
      "model = extra-trees score = 0.43\n",
      "time:  14.34\n",
      "----------\n",
      "model = lightgbm score = 0.7\n",
      "time:  20.07\n",
      "----------\n",
      "model = catboost score = 0.73\n",
      "time:  142.91\n",
      "----------\n",
      "model = xgboost score = 0.73\n",
      "time:  162.38\n",
      "----------\n",
      "94/108\n",
      "max_features = 1000 tokenizer=polish_tokenizer_md, vectorizer=TfidfVectorizer, stop_words = 381, dfs=min:0.1, max:0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['cal', 'chcieƒá', 'czas', 'kierunek', 'mieƒá', 'mo≈ºliwy', 'musiƒá', 'm√≥c', 'rok', 'wiedzieƒá'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Vectorization time: 154.7251739501953\n",
      "####################\n",
      "model = decision tree score = 0.48\n",
      "time:  0.82\n",
      "----------\n",
      "model = random forest score = 0.55\n",
      "time:  5.7\n",
      "----------\n",
      "model = extra-trees score = 0.48\n",
      "time:  3.56\n",
      "----------\n",
      "model = lightgbm score = 0.6\n",
      "time:  6.46\n",
      "----------\n",
      "model = catboost score = 0.61\n",
      "time:  14.17\n",
      "----------\n",
      "model = xgboost score = 0.61\n",
      "time:  13.24\n",
      "----------\n",
      "95/108\n",
      "max_features = 1000 tokenizer=polish_tokenizer_md, vectorizer=TfidfVectorizer, stop_words = 381, dfs=min:0.1, max:0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['cal', 'chcieƒá', 'czas', 'kierunek', 'mieƒá', 'mo≈ºliwy', 'musiƒá', 'm√≥c', 'rok', 'wiedzieƒá'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Vectorization time: 152.89959383010864\n",
      "####################\n",
      "model = decision tree score = 0.52\n",
      "time:  0.86\n",
      "----------\n",
      "model = random forest score = 0.56\n",
      "time:  5.88\n",
      "----------\n",
      "model = extra-trees score = 0.47\n",
      "time:  3.7\n",
      "----------\n",
      "model = lightgbm score = 0.61\n",
      "time:  6.87\n",
      "----------\n",
      "model = catboost score = 0.62\n",
      "time:  14.66\n",
      "----------\n",
      "model = xgboost score = 0.61\n",
      "time:  12.3\n",
      "----------\n",
      "96/108\n",
      "max_features = 1000 tokenizer=polish_tokenizer_md, vectorizer=TfidfVectorizer, stop_words = 381, dfs=min:0.1, max:1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['cal', 'chcieƒá', 'czas', 'kierunek', 'mieƒá', 'mo≈ºliwy', 'musiƒá', 'm√≥c', 'rok', 'wiedzieƒá'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Vectorization time: 154.52091884613037\n",
      "####################\n",
      "model = decision tree score = 0.53\n",
      "time:  0.93\n",
      "----------\n",
      "model = random forest score = 0.57\n",
      "time:  6.23\n",
      "----------\n",
      "model = extra-trees score = 0.47\n",
      "time:  3.71\n",
      "----------\n",
      "model = lightgbm score = 0.62\n",
      "time:  7.88\n",
      "----------\n",
      "model = catboost score = 0.62\n",
      "time:  15.85\n",
      "----------\n",
      "model = xgboost score = 0.62\n",
      "time:  14.14\n",
      "----------\n",
      "97/108\n",
      "max_features = 1000 tokenizer=polish_tokenizer_md, vectorizer=TfidfVectorizer, stop_words = 381, dfs=min:1, max:0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['cal', 'chcieƒá', 'czas', 'kierunek', 'mieƒá', 'mo≈ºliwy', 'musiƒá', 'm√≥c', 'rok', 'wiedzieƒá'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Vectorization time: 160.47012066841125\n",
      "####################\n",
      "model = decision tree score = 0.48\n",
      "time:  7.94\n",
      "----------\n",
      "model = random forest score = 0.53\n",
      "time:  13.08\n",
      "----------\n",
      "model = extra-trees score = 0.45\n",
      "time:  14.17\n",
      "----------\n",
      "model = lightgbm score = 0.7\n",
      "time:  20.54\n",
      "----------\n",
      "model = catboost score = 0.72\n",
      "time:  143.96\n",
      "----------\n",
      "model = xgboost score = 0.73\n",
      "time:  163.82\n",
      "----------\n",
      "98/108\n",
      "max_features = 1000 tokenizer=polish_tokenizer_md, vectorizer=TfidfVectorizer, stop_words = 381, dfs=min:1, max:0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['cal', 'chcieƒá', 'czas', 'kierunek', 'mieƒá', 'mo≈ºliwy', 'musiƒá', 'm√≥c', 'rok', 'wiedzieƒá'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Vectorization time: 155.87742042541504\n",
      "####################\n",
      "model = decision tree score = 0.47\n",
      "time:  7.89\n",
      "----------\n",
      "model = random forest score = 0.54\n",
      "time:  12.33\n",
      "----------\n",
      "model = extra-trees score = 0.43\n",
      "time:  14.15\n",
      "----------\n",
      "model = lightgbm score = 0.7\n",
      "time:  19.57\n",
      "----------\n",
      "model = catboost score = 0.72\n",
      "time:  142.85\n",
      "----------\n",
      "model = xgboost score = 0.73\n",
      "time:  160.98\n",
      "----------\n",
      "99/108\n",
      "max_features = 1000 tokenizer=polish_tokenizer_md, vectorizer=TfidfVectorizer, stop_words = 381, dfs=min:1, max:1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['cal', 'chcieƒá', 'czas', 'kierunek', 'mieƒá', 'mo≈ºliwy', 'musiƒá', 'm√≥c', 'rok', 'wiedzieƒá'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Vectorization time: 151.5137951374054\n",
      "####################\n",
      "model = decision tree score = 0.48\n",
      "time:  8.58\n",
      "----------\n",
      "model = random forest score = 0.53\n",
      "time:  12.98\n",
      "----------\n",
      "model = extra-trees score = 0.46\n",
      "time:  15.13\n",
      "----------\n",
      "model = lightgbm score = 0.7\n",
      "time:  19.89\n",
      "----------\n",
      "model = catboost score = 0.73\n",
      "time:  145.43\n",
      "----------\n",
      "model = xgboost score = 0.73\n",
      "time:  198.56\n",
      "----------\n",
      "100/108\n",
      "max_features = 1000 tokenizer=polish_tokenizer_md, vectorizer=TfidfVectorizer, stop_words = 0, dfs=min:0.01, max:0.3\n",
      "####################\n",
      "Vectorization time: 164.6885859966278\n",
      "####################\n",
      "model = decision tree score = 0.48\n",
      "time:  9.56\n",
      "----------\n",
      "model = random forest score = 0.53\n",
      "time:  14.43\n",
      "----------\n",
      "model = extra-trees score = 0.44\n",
      "time:  16.73\n",
      "----------\n",
      "model = lightgbm score = 0.71\n",
      "time:  30.48\n",
      "----------\n",
      "model = catboost score = 0.73\n",
      "time:  197.13\n",
      "----------\n",
      "model = xgboost score = 0.74\n",
      "time:  197.7\n",
      "----------\n",
      "101/108\n",
      "max_features = 1000 tokenizer=polish_tokenizer_md, vectorizer=TfidfVectorizer, stop_words = 0, dfs=min:0.01, max:0.5\n",
      "####################\n",
      "Vectorization time: 168.5904667377472\n",
      "####################\n",
      "model = decision tree score = 0.53\n",
      "time:  8.9\n",
      "----------\n",
      "model = random forest score = 0.54\n",
      "time:  14.58\n",
      "----------\n",
      "model = extra-trees score = 0.46\n",
      "time:  15.82\n",
      "----------\n",
      "model = lightgbm score = 0.71\n",
      "time:  36.26\n",
      "----------\n",
      "model = catboost score = 0.74\n",
      "time:  201.41\n",
      "----------\n",
      "model = xgboost score = 0.74\n",
      "time:  191.38\n",
      "----------\n",
      "102/108\n",
      "max_features = 1000 tokenizer=polish_tokenizer_md, vectorizer=TfidfVectorizer, stop_words = 0, dfs=min:0.01, max:1.0\n",
      "####################\n",
      "Vectorization time: 163.75558924674988\n",
      "####################\n",
      "model = decision tree score = 0.59\n",
      "time:  10.02\n",
      "----------\n",
      "model = random forest score = 0.61\n",
      "time:  15.75\n",
      "----------\n",
      "model = extra-trees score = 0.48\n",
      "time:  16.09\n",
      "----------\n",
      "model = lightgbm score = 0.73\n",
      "time:  44.24\n",
      "----------\n",
      "model = catboost score = 0.74\n",
      "time:  215.47\n",
      "----------\n",
      "model = xgboost score = 0.75\n",
      "time:  207.38\n",
      "----------\n",
      "103/108\n",
      "max_features = 1000 tokenizer=polish_tokenizer_md, vectorizer=TfidfVectorizer, stop_words = 0, dfs=min:0.1, max:0.3\n",
      "####################\n",
      "Vectorization time: 165.88639307022095\n",
      "####################\n",
      "model = decision tree score = 0.49\n",
      "time:  1.49\n",
      "----------\n",
      "model = random forest score = 0.56\n",
      "time:  7.8\n",
      "----------\n",
      "model = extra-trees score = 0.46\n",
      "time:  4.56\n",
      "----------\n",
      "model = lightgbm score = 0.64\n",
      "time:  14.28\n",
      "----------\n",
      "model = catboost score = 0.66\n",
      "time:  34.71\n",
      "----------\n",
      "model = xgboost score = 0.64\n",
      "time:  27.86\n",
      "----------\n",
      "104/108\n",
      "max_features = 1000 tokenizer=polish_tokenizer_md, vectorizer=TfidfVectorizer, stop_words = 0, dfs=min:0.1, max:0.5\n",
      "####################\n",
      "Vectorization time: 167.8594455718994\n",
      "####################\n",
      "model = decision tree score = 0.53\n",
      "time:  1.82\n",
      "----------\n",
      "model = random forest score = 0.6\n",
      "time:  9.29\n",
      "----------\n",
      "model = extra-trees score = 0.48\n",
      "time:  4.92\n",
      "----------\n",
      "model = lightgbm score = 0.66\n",
      "time:  20.76\n",
      "----------\n",
      "model = catboost score = 0.67\n",
      "time:  45.55\n",
      "----------\n",
      "model = xgboost score = 0.66\n",
      "time:  38.87\n",
      "----------\n",
      "105/108\n",
      "max_features = 1000 tokenizer=polish_tokenizer_md, vectorizer=TfidfVectorizer, stop_words = 0, dfs=min:0.1, max:1.0\n",
      "####################\n",
      "Vectorization time: 164.9328830242157\n",
      "####################\n",
      "model = decision tree score = 0.57\n",
      "time:  2.16\n",
      "----------\n",
      "model = random forest score = 0.63\n",
      "time:  10.51\n",
      "----------\n",
      "model = extra-trees score = 0.52\n",
      "time:  4.97\n",
      "----------\n",
      "model = lightgbm score = 0.69\n",
      "time:  19.04\n",
      "----------\n",
      "model = catboost score = 0.7\n",
      "time:  42.45\n",
      "----------\n",
      "model = xgboost score = 0.7\n",
      "time:  36.95\n",
      "----------\n",
      "106/108\n",
      "max_features = 1000 tokenizer=polish_tokenizer_md, vectorizer=TfidfVectorizer, stop_words = 0, dfs=min:1, max:0.3\n",
      "####################\n",
      "Vectorization time: 168.53897976875305\n",
      "####################\n",
      "model = decision tree score = 0.48\n",
      "time:  9.68\n",
      "----------\n",
      "model = random forest score = 0.54\n",
      "time:  14.78\n",
      "----------\n",
      "model = extra-trees score = 0.44\n",
      "time:  18.46\n",
      "----------\n",
      "model = lightgbm score = 0.71\n",
      "time:  40.46\n",
      "----------\n",
      "model = catboost score = 0.72\n",
      "time:  206.41\n",
      "----------\n",
      "model = xgboost score = 0.74\n",
      "time:  182.84\n",
      "----------\n",
      "107/108\n",
      "max_features = 1000 tokenizer=polish_tokenizer_md, vectorizer=TfidfVectorizer, stop_words = 0, dfs=min:1, max:0.5\n",
      "####################\n",
      "Vectorization time: 152.2227656841278\n",
      "####################\n",
      "model = decision tree score = 0.53\n",
      "time:  8.14\n",
      "----------\n",
      "model = random forest score = 0.55\n",
      "time:  13.13\n",
      "----------\n",
      "model = extra-trees score = 0.45\n",
      "time:  14.74\n",
      "----------\n",
      "model = lightgbm score = 0.72\n",
      "time:  29.55\n",
      "----------\n",
      "model = catboost score = 0.73\n",
      "time:  166.11\n",
      "----------\n",
      "model = xgboost score = 0.74\n",
      "time:  225.21\n",
      "----------\n",
      "108/108\n",
      "max_features = 1000 tokenizer=polish_tokenizer_md, vectorizer=TfidfVectorizer, stop_words = 0, dfs=min:1, max:1.0\n",
      "####################\n",
      "Vectorization time: 182.6332654953003\n",
      "####################\n",
      "model = decision tree score = 0.58\n",
      "time:  10.04\n",
      "----------\n",
      "model = random forest score = 0.58\n",
      "time:  16.92\n",
      "----------\n",
      "model = extra-trees score = 0.47\n",
      "time:  19.45\n",
      "----------\n",
      "model = lightgbm score = 0.73\n",
      "time:  62.47\n",
      "----------\n",
      "model = catboost score = 0.75\n",
      "time:  225.93\n",
      "----------\n",
      "model = xgboost score = 0.75\n",
      "time:  211.21\n",
      "----------\n",
      "==========\n",
      "Start of experiment: 2021-10-24 06:48:43.279073\n",
      "End of experiment: 2021-10-24 19:44:56.046525\n",
      "Experiment took:\n",
      "    0 days\n",
      "    12 hours\n",
      "    56 minutes\n",
      "    12 seconds\n"
     ]
    }
   ],
   "source": [
    "params_cnt = len(max_features_list)*\\\n",
    "len(tokenizer_list)*\\\n",
    "len(stop_words_list)*\\\n",
    "len(vectorizers_list)*\\\n",
    "len(min_df_list)*len(max_df_list)\n",
    "\n",
    "exp_start = datetime.datetime.now()\n",
    "\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=0) \n",
    "\n",
    "for n,(Vectorizer, max_features, tokenizer, stop_words, min_df, max_df) in \\\n",
    "enumerate(product(vectorizers_list, max_features_list, tokenizer_list, stop_words_list, min_df_list, max_df_list),1):\n",
    "    \n",
    "    model_str = f'max_features = {max_features} tokenizer={tokenizer.__name__}, vectorizer={Vectorizer.__name__}, stop_words = {len(stop_words)}, dfs=min:{min_df}, max:{max_df}'\n",
    "    \n",
    "    print(f'{n}/{params_cnt}')\n",
    "    print(model_str)\n",
    "    \n",
    "    kwargs = {'scoring': ['f1_micro']}\n",
    "    \n",
    "    \n",
    "    vectorizer_kwargs = {'max_features': max_features,\n",
    "                         'tokenizer': tokenizer,\n",
    "                         'stop_words': stop_words,\n",
    "                         'min_df':min_df,\n",
    "                         'max_df':max_df}\n",
    "    vec = Vectorizer(**vectorizer_kwargs)\n",
    "\n",
    "    start = time.time()\n",
    "    X = vec.fit_transform(SELECTED_X).toarray()\n",
    "    end = time.time()\n",
    "    print('##'*10)\n",
    "    print(f'Vectorization time: {end-start}')\n",
    "    print('##'*10)\n",
    "\n",
    "    for model_name, model_obj in models_list:\n",
    "        with mlflow.start_run(experiment_id=th._eid(EXP_NAME), run_name=f'{model_str}-{model_name}'):\n",
    "            start = time.time()\n",
    "            scores = cross_validate(model_obj, X, y, cv=cv, scoring=kwargs['scoring'], return_train_score=True)    \n",
    "            #print(scores)\n",
    "            mean = np.around( np.mean(scores['test_f1_micro']), 2)\n",
    "            std = np.around(np.std(scores['test_f1_micro']),2)\n",
    "\n",
    "            print(f'model = {model_name} score = {mean}')\n",
    "\n",
    "            y_pred = cross_val_predict(model_obj, X, y, cv=cv)\n",
    "\n",
    "            #fig = plt.subplots(figsize=(10, 10))\n",
    "            fig = plt.figure()\n",
    "            fig = plot_confusion_matrix(y, y_pred, title='model: {}'.format(model_name))\n",
    "            plt.savefig(\"confusion_matrix.png\")\n",
    "            \n",
    "            fig = plt.figure()\n",
    "            fig = plot_learning_curve(model_obj, X, y, title='model: {}'.format(model_name))\n",
    "            plt.savefig(\"learning_curve.png\")\n",
    "\n",
    "            end = time.time()\n",
    "            print('time: ',round((end-start),2))\n",
    "            print('-'*10)\n",
    "\n",
    "            # log params\n",
    "            mlflow.log_param('model', model_name)\n",
    "            mlflow.log_param('max_features', max_features)\n",
    "            mlflow.log_param('tokenizer', tokenizer.__name__)\n",
    "            mlflow.log_param('vectorizer', Vectorizer.__name__)\n",
    "            mlflow.log_param('lenstop_words', len(stop_words))\n",
    "            mlflow.log_param('min_df', min_df)\n",
    "            mlflow.log_param('max_df', max_df)\n",
    "\n",
    "            # log metrics\n",
    "            mlflow.log_metric('mean_train_f1_micro', np.around( np.mean(scores['train_f1_micro']), 2))\n",
    "            mlflow.log_metric('mean_test_f1_micro', np.around( np.mean(scores['test_f1_micro']), 2))\n",
    "\n",
    "            mlflow.log_metric('std_train_f1_micro', np.around( np.std(scores['train_f1_micro']), 2))\n",
    "            mlflow.log_metric('std_test_f1_micro', np.around( np.std(scores['test_f1_micro']), 2))\n",
    "            \n",
    "            mlflow.log_metric('calc_time', (end-start))\n",
    "\n",
    "            # log artifacts\n",
    "            mlflow.log_artifact(\"confusion_matrix.png\")\n",
    "            mlflow.log_artifact(\"learning_curve.png\")\n",
    "            #plt.show()\n",
    "\n",
    "\n",
    "exp_end = datetime.datetime.now()\n",
    "print('='*10)\n",
    "print(f'Start of experiment: {exp_start}')\n",
    "print(f'End of experiment: {exp_end}')\n",
    "th.calculate_delta((exp_end-exp_start))\n",
    "\n",
    "dfruns = mlflow.search_runs(experiment_ids=[th._eid(EXP_NAME)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mlflow ui --backend-store-uri /home/jovyan/nlp2/shared-mlruns/team-three/mariusz --default-artifact-root /home/jovyan/nlp2/shared-mlruns/team-three/mariusz --port 5001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-10-24 06:47:24 +0000] [623] [INFO] Starting gunicorn 20.1.0\n",
      "[2021-10-24 06:47:24 +0000] [623] [INFO] Listening at: http://127.0.0.1:5000 (623)\n",
      "[2021-10-24 06:47:24 +0000] [623] [INFO] Using worker: sync\n",
      "[2021-10-24 06:47:24 +0000] [625] [INFO] Booting worker with pid: 625\n",
      "^C\n",
      "[2021-10-24 06:48:21 +0000] [623] [INFO] Handling signal: int\n",
      "[2021-10-24 06:48:21 +0000] [625] [INFO] Worker exiting (pid: 625)\n"
     ]
    }
   ],
   "source": [
    "!mlflow ui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
